{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "#!pip install tensorflow\n",
    "#!pip install tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPoseHand(npy):\n",
    "    np_matrix=npy.reshape(-1,1629)\n",
    "    # arr = []\n",
    "    # print(np_matrix.shape)\n",
    "    pose  = np_matrix[: ,0:99]\n",
    "    hands  = np_matrix[: ,1503:1629]\n",
    "    \n",
    " \n",
    "    arr = np.concatenate((pose, hands), axis=1)\n",
    "\n",
    "\n",
    "      \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# crossValidationDataPaths=['NORMALIZED/NumericalData/ML/Training/RightHand','NORMALIZED/NumericalData/ML/Training/LeftHand']\n",
    "# testPaths=['NORMALIZED/NumericalData/ML/Test/RightHand','NORMALIZED/NumericalData/ML/Test/LeftHand']\n",
    "\n",
    "crossValidationDataPaths=['NORMALIZED_ORIGINAL/NumericalData/ML/Training/RightHand','NORMALIZED_ORIGINAL/NumericalData/ML/Training/LeftHand_NO_FLIP']\n",
    "testPaths=['NORMALIZED_ORIGINAL/NumericalData/ML/Test/RightHand','NORMALIZED_ORIGINAL/NumericalData/ML/Test/LeftHand_NO_FLIP']\n",
    "\n",
    "\n",
    "\n",
    "ml_instances_paths=[]\n",
    "for path in crossValidationDataPaths:\n",
    "    #print(path)\n",
    "    classFolders= os.listdir(path)\n",
    "    for classFolder in classFolders:\n",
    "        classFolder_path=f'{path}/{classFolder}'\n",
    "        classTrials =os.listdir(classFolder_path)\n",
    "        for trial in classTrials:\n",
    "            trialPath =f'{classFolder_path}/{trial}'\n",
    "            #print(trialPath)\n",
    "            ml_instances_paths.append(trialPath)\n",
    "            \n",
    "\n",
    "\n",
    "#randomize the list\n",
    "random.shuffle(ml_instances_paths)\n",
    "#random.shuffle(ml_instances_paths)\n",
    "#random.shuffle(ml_instances_paths)\n",
    "\n",
    "\n",
    "x_shape =[]\n",
    "y_shape =[]\n",
    "for ml_instances_path in ml_instances_paths:\n",
    "    label = ml_instances_path.split('/')\n",
    "    label = label[len(label)-1].split('_')[0]\n",
    "    #print(label)\n",
    "    y_shape.append(int(label))\n",
    "    npy =np.load(ml_instances_path)\n",
    "    npy = npy*100\n",
    "  \n",
    "    # npy_matrix=npy.reshape(-1,1629)\n",
    "   \n",
    "    npy_matrix=extractPoseHand(npy)\n",
    "    x_shape.append(npy_matrix)    \n",
    "#print(npy_matrix)\n",
    "#print(len(x_shape[3]))\n",
    "#print(len(x_shape))\n",
    "#print(len(y_shape))\n",
    "    \n",
    "\n",
    "\n",
    "#testPaths=['NumericalData/ML_PROLONG/Test/RightHand','NumericalData/ML_PROLONG/Test/LeftHand']\n",
    "#testPaths=['NumericalData/ML_PROLONG/Test/RightHand','NumericalData/ML_PROLONG/Test/LeftHand_NO_FLIP']\n",
    "#testPaths=['NumericalData/ML/Test/RightHand']\n",
    "\n",
    "#testPaths=['NumericalData/ML3CLASSES/Test/RightHand','NumericalData/ML3CLASSES/Test/LeftHand']\n",
    "#testPaths=['NumericalData/ML3C_PROLONG/Test/RightHand','NumericalData/ML3C_PROLONG/Test/LeftHand']\n",
    "#testPaths=['NumericalData/ML3CLASSES/Test/RightHand']\n",
    "test_instances_paths=[]\n",
    "for path in testPaths:\n",
    "    #print(path)\n",
    "    testFolders= os.listdir(path)\n",
    "    for testFolder in testFolders:\n",
    "        testFolder_path=f'{path}/{testFolder}'\n",
    "        testTrials =os.listdir(testFolder_path)\n",
    "        for trial in testTrials:\n",
    "            trialPath =f'{testFolder_path}/{trial}'\n",
    "            #print(trialPath)\n",
    "            test_instances_paths.append(trialPath)\n",
    "\n",
    "\n",
    "test_x_shape =[]\n",
    "test_y_shape =[]\n",
    "for test_instances_path in test_instances_paths:\n",
    "    label = test_instances_path.split('/')\n",
    "    label = label[len(label)-1].split('_')[0]\n",
    "    #print(label)\n",
    "    test_y_shape.append(int(label))\n",
    "    npy =np.load(test_instances_path)\n",
    "    npy=npy*100\n",
    "    # npy_matrix=npy.reshape(-1,1629)\n",
    "\n",
    "    npy_matrix=extractPoseHand(npy)\n",
    "    \n",
    "    test_x_shape.append(npy_matrix)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_shape.shape)\n",
    "# print(y_shape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_x_shape.shape)\n",
    "# print(test_y_shape.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 11:46:18.517672: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-30 11:46:18.538926: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-30 11:46:18.538947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-30 11:46:18.539502: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-30 11:46:18.551832: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-30 11:46:18.949171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 4, 39, 34, 36, 56, 10, 56, 31, 21, 58, 7, 53, 30, 55, 43, 0, 56, 15, 55, 1, 50, 36, 15, 25, 55, 17, 40, 59, 20, 1, 31, 57, 2, 39, 51, 56, 13, 49, 7, 1, 38, 7, 21, 29, 51, 26, 14, 54, 35, 27, 33, 27, 45, 6, 44, 59, 6, 2, 53, 2, 56, 10, 58, 30, 8, 1, 22, 4, 18, 25, 11, 47, 35, 51, 18, 34, 55, 24, 23, 3, 39, 44, 33, 19, 50, 54, 45, 34, 52, 24, 34, 44, 23, 6, 55, 15, 1, 34, 16, 11, 57, 46, 19, 15, 58, 13, 12, 28, 4, 5, 24, 54, 4, 19, 49, 14, 46, 19, 57, 27, 46, 51, 33, 50, 53, 22, 5, 58, 15, 21, 33, 57, 41, 51, 15, 43, 52, 22, 28, 2, 18, 12, 57, 28, 8, 30, 2, 29, 6, 43, 4, 27, 57, 6, 12, 9, 28, 46, 59, 8, 52, 57, 11, 7, 6, 51, 12, 5, 53, 54, 33, 6, 29, 33, 9, 11, 29, 6, 58, 10, 7, 27, 53, 11, 46, 49, 43, 29, 57, 7, 16, 19, 52, 59, 7, 51, 57, 10, 39, 33, 8, 11, 1, 33, 6, 54, 26, 45, 52, 8, 49, 50, 58, 39, 48, 56, 41, 58, 14, 16, 52, 56, 48, 44, 54, 16, 17, 52, 14, 19, 18, 58, 50, 51, 4, 44, 13, 5, 5, 51, 48, 59, 3, 51, 16, 11, 5, 8, 36, 58, 16, 43, 19, 44, 47, 59, 33, 18, 34, 57, 9, 50, 53, 17, 7, 15, 45, 35, 35, 55, 9, 42, 4, 44, 19, 12, 7, 52, 37, 25, 43, 12, 6, 22, 51, 24, 28, 44, 44, 51, 39, 47, 18, 54, 43, 50, 0, 9, 52, 30, 31, 5, 34, 36, 15, 20, 57, 49, 21, 0, 16, 42, 27, 39, 52, 55, 32, 59, 6, 36, 31, 15, 50, 12, 9, 17, 26, 6, 15, 21, 53, 11, 26, 8, 0, 46, 59, 16, 27, 39, 54, 2, 40, 18, 32, 34, 9, 51, 18, 57, 20, 15, 39, 47, 54, 33, 57, 23, 35, 26, 46, 37, 5, 53, 18, 33, 4, 9, 58, 21, 53, 25, 26, 25, 36, 22, 39, 58, 1, 7, 19, 17, 26, 45, 20, 40, 43, 32, 31, 10, 56, 57, 42, 58, 6, 29, 8, 48, 17, 6, 4, 57, 40, 2, 13, 34, 53, 22, 50, 51, 12, 0, 21, 52, 59, 35, 23, 40, 19, 43, 4, 4, 54, 29, 19, 10, 54, 21, 54, 21, 1, 8, 31, 34, 17, 16, 29, 9, 33, 31, 41, 38, 20, 41, 29, 37, 54, 36, 54, 19, 27, 35, 11, 5, 30, 22, 10, 0, 2, 43, 59, 31, 58, 56, 24, 8, 35, 2, 17, 39, 53, 27, 31, 6, 20, 1, 44, 5, 9, 28, 35, 13, 39, 30, 3, 52, 44, 52, 8, 25, 55, 35, 56, 54, 25, 19, 27, 4, 12, 37, 1, 9, 13, 8, 24, 56, 45, 42, 4, 51, 21, 50, 57, 13, 15, 13, 0, 22, 58, 24, 49, 21, 8, 16, 46, 51, 2, 46, 7, 27, 57, 52, 0, 56, 34, 8, 48, 3, 30, 1, 36, 23, 57, 52, 50, 17, 46, 47, 23, 13, 34, 32, 10, 57, 25, 37, 55, 54, 21, 5, 43, 25, 15, 52, 56, 56, 8, 25, 18, 59, 32, 17, 14, 7, 18, 10, 2, 3, 18, 35, 42, 4, 51, 22, 58, 21, 26, 33, 19, 27, 8, 25, 50, 25, 42, 59, 2, 13, 37, 57, 44, 37, 37, 21, 57, 30, 54, 7, 19, 0, 11, 36, 3, 18, 3, 6, 34, 0, 48, 23, 46, 13, 59, 49, 14, 45, 18, 19, 13, 43, 8, 32, 50, 19, 3, 34, 20, 16, 50, 18, 57, 23, 34, 43, 40, 50, 9, 40, 1, 4, 59, 39, 16, 26, 31, 21, 5, 56, 3, 26, 49, 21, 27, 58, 20, 55, 16, 6, 8, 24, 49, 23, 29, 11, 28, 24, 17, 29, 14, 15, 53, 45, 7, 45, 4, 21, 49, 59, 6, 18, 49, 50, 53, 19, 8, 27, 52, 23, 32, 4, 1, 4, 7, 27, 12, 6, 47, 53, 50, 56, 24, 12, 4, 55, 39, 19, 58, 23, 46, 24, 16, 55, 2, 36, 53, 59, 3, 7, 22, 33, 18, 6, 40, 32, 14, 44, 34, 51, 11, 12, 6, 16, 7, 11, 55, 22, 11, 26, 53, 58, 58, 32, 12, 43, 17, 2, 37, 42, 53, 8, 14, 0, 57, 53, 53, 49, 48, 46, 22, 38, 14, 29, 43, 16, 0, 35, 4, 51, 2, 47, 19, 51, 51, 4, 3, 42, 56, 43, 25, 16, 4, 0, 40, 24, 2, 57, 1, 20, 13, 6, 29, 14, 27, 34, 14, 25, 31, 17, 23, 51, 10, 6, 39, 45, 1, 20, 41, 18, 9, 13, 59, 47, 17, 23, 58, 24, 59, 14, 52, 55, 18, 5, 52, 53, 7, 40, 6, 36, 8, 10, 58, 3, 57, 3, 20, 31, 28, 53, 46, 58, 50, 31, 51, 0, 45, 0, 52, 18, 20, 46, 16, 15, 53, 57, 51, 41, 8, 51, 10, 48, 35, 56, 7, 10, 22, 5, 8, 10, 28, 33, 51, 11, 50, 17, 4, 47, 22, 6, 5, 3, 8, 14, 34, 4, 49, 5, 56, 57, 5, 22, 45, 23, 22, 15, 16, 50, 59, 14, 55, 36, 15, 23, 52, 49, 51, 31, 44, 55, 4, 27, 17, 13, 5, 7, 20, 25, 12, 13, 44, 26, 16, 39, 33, 7, 0, 16, 16, 50, 52, 46, 44, 52, 2, 34, 23, 0, 53, 31, 47, 7, 58, 6, 51, 45, 14, 8, 42, 29, 45, 21, 12, 7, 12, 57, 46, 7, 41, 22, 3, 33, 47, 58, 21, 55, 13, 12, 54, 55, 19, 31, 13, 26, 36, 3, 3, 12, 27, 33, 52, 52, 43, 47, 14, 1, 43, 54, 34, 57, 37, 4, 5, 9, 3, 27, 43, 18, 29, 45, 15, 0, 40, 48, 25, 17, 17, 14, 59, 4, 39, 23, 52, 47, 2, 36, 27, 12, 54, 8, 43, 16, 7, 5, 14, 27, 19, 46, 13, 32, 56, 31, 11, 56, 21, 22, 12, 37, 17, 43, 27, 50, 50, 21, 2, 0, 16, 57, 56, 10, 56, 8, 48, 35, 57, 56, 42, 8, 45, 8, 12, 43, 15, 42, 48, 3, 11, 11, 55, 43, 35, 28, 23, 0, 26, 55, 3, 18, 18, 20, 55, 20, 25, 29, 59, 40, 5, 21, 29, 33, 21, 11, 58, 32, 59, 25, 6, 28, 13, 49, 7, 43, 59, 16, 58, 48, 7, 42, 2, 2, 2, 50, 37, 21, 20, 31, 4, 11, 4, 19, 29, 7, 1, 45, 5, 23, 31, 40, 56, 22, 52, 57, 58, 53, 30, 51, 56, 4, 16, 21, 50, 40, 5, 15, 53, 30, 51, 35, 47, 48, 19, 2, 12, 2, 56, 12, 48, 15, 30, 33, 45, 31, 9, 39, 24, 6, 31, 12, 45, 7, 0, 52, 23, 52, 21, 2, 36, 50, 57, 13, 28, 49, 7, 52, 19, 4, 16, 16, 26, 52, 30, 6, 20, 2, 48, 42, 24, 3, 10, 54, 43, 26, 27, 23, 14, 51, 18, 0, 24, 51, 33, 20, 10, 35, 7, 0, 59, 52, 23, 16, 32, 27, 58, 49, 18, 40, 28, 33, 0, 0, 9, 40, 14, 50, 17, 35, 45, 53, 16, 27, 21, 24, 14, 10, 23, 22, 41, 14, 48, 50, 18, 23, 32, 27, 23, 43, 3, 10, 58, 28, 24, 53, 8, 33, 35, 0, 53, 55, 57, 48, 12, 14, 32, 44, 58, 12, 49, 31, 29, 43, 26, 56, 24, 9, 43, 9, 26, 6, 43, 51, 28, 59, 36, 53, 13, 28, 12, 52, 44, 44, 10, 51, 37, 53, 53, 25, 25, 15, 20, 33, 29, 17, 3, 53, 59, 59, 57, 42, 43, 15, 19, 20, 17, 47, 13, 10, 37, 55, 6, 50, 2, 28, 48, 56, 4, 6, 9, 42, 23, 4, 6, 22, 53, 45, 6, 12, 2, 8, 34, 21, 55, 1, 10, 17, 0, 20, 8, 54, 16, 34, 25, 3, 3, 18, 25, 0, 56, 23, 24, 59, 46, 11, 25, 3, 8, 34, 50, 1, 4, 25, 50, 22, 32, 44, 13, 46, 23, 5, 35, 30, 57, 50, 33, 54, 34, 16, 19, 53, 34, 39, 52, 5, 46, 17, 43, 32, 18, 2, 51, 56, 16, 11, 11, 58, 30, 4, 58, 25, 19, 53, 1, 57, 30, 24, 51, 53, 22, 23, 57, 39, 39, 58, 7, 17, 3, 33, 35, 47, 18, 52, 58, 48, 46, 56, 52, 52, 58, 31, 14, 40, 37, 17, 26, 55, 18, 12, 39, 15, 35, 4, 38, 58, 25, 16, 50, 44, 32, 20, 7, 18, 23, 15, 56, 33, 21, 41, 46, 32, 13, 6, 36, 40, 20, 34, 22, 0, 53, 44, 4, 2, 30, 52, 18, 16, 1, 53, 42, 52, 42, 28, 20, 25, 6, 14, 2, 59, 54, 18, 24, 49, 55, 51, 3, 8, 45, 50, 38, 58, 31, 17, 50, 13, 59, 39, 27, 3, 17, 41, 11, 39, 37, 20, 23, 38, 42, 53, 9, 58, 50, 12, 1, 3, 22, 24, 55, 59, 52, 46, 47, 33, 53, 44, 1, 4, 16, 25, 57, 35, 44, 16, 55, 35, 59, 59, 54, 51, 16, 8, 37, 51, 27, 1, 35, 21, 2, 12, 19, 31, 12, 51, 0, 21, 56, 35, 16, 11, 33, 9, 58, 37, 47, 21, 13, 9, 56, 47, 44, 55, 9, 52, 58, 21, 6, 0, 7, 59, 11, 52, 6, 21, 55, 51, 17, 44, 29, 5, 2, 1, 47, 6, 26, 4, 16, 47, 14, 52, 38, 19, 4, 43, 34, 18, 23, 58, 57, 56, 17, 24, 54, 17, 24, 16, 42, 58, 3, 29, 30, 37, 20, 9, 33, 44, 4, 39, 25, 54, 15, 10, 24, 49, 29, 15, 55, 57, 13, 8, 55, 57, 46, 49, 34, 55, 33, 21, 29, 14, 55, 52, 8, 40, 59, 16, 3, 2, 53, 31, 45, 25, 51, 57, 18, 21, 51, 19, 16, 8, 52, 40, 57, 49, 25, 51, 28, 28, 28, 6, 56, 18, 15, 24, 48, 59, 10, 58, 56, 1, 14, 21, 5, 13, 53, 57, 25, 41, 14, 1, 35, 8, 49, 25, 47, 5, 29, 45, 46, 21, 40, 7, 58, 57, 30, 48, 28, 20, 3, 31, 18, 41, 14, 18, 59, 22, 59, 4, 1, 15, 23, 56, 53, 51, 5, 1, 15, 27, 25, 56, 54, 47, 4, 29, 46, 8, 0, 27, 11, 51, 25, 10, 48, 59, 50, 54, 30, 0, 55, 58, 59, 38, 40, 18, 6, 12, 2, 36, 27, 26, 49, 32, 40, 25, 3, 55, 29, 38, 56, 2, 20, 40, 39, 6, 19, 13, 17, 13, 56, 4, 50, 52, 53, 12, 40, 31, 54, 51, 41, 45, 41, 58, 58, 45, 14, 59, 17, 33, 18, 6, 23, 45, 50, 19, 10, 26, 52, 50, 22, 48, 28, 59, 19, 14, 1, 55, 33, 47, 17, 25, 29, 0, 28, 31, 56, 30, 52, 36, 7, 56, 29, 30, 17, 56, 55, 49, 59, 18, 32, 47, 58, 29, 26, 56, 41, 57, 18, 6, 58, 50, 4, 1, 23, 38, 54, 4, 34, 13, 57, 47, 5, 53, 58, 8, 24, 32, 31, 34, 17, 15, 18, 24, 11, 41, 38, 3, 31, 44, 56, 24, 17, 20, 59, 3, 27, 3, 43, 11, 51, 59, 45, 59, 12, 22, 52, 47, 28, 37, 44, 47, 24, 21, 58, 29, 41, 28, 45, 35, 52, 6, 26, 37, 37, 0, 59, 19, 57, 6, 45, 42, 30, 25, 57, 19, 33, 2, 13, 16, 56, 25, 51, 30, 6, 28, 56, 55, 55, 15, 34, 9, 11, 54, 46, 42, 1, 48, 59, 37, 48, 16, 41, 28, 32, 14, 11, 32, 3, 41, 49, 54, 16, 9, 6, 44, 45, 9, 12, 26, 41, 38, 8, 23, 42, 15, 3, 39, 20, 12, 25, 37, 51, 32, 53, 51, 48, 6, 43, 39, 10, 26, 45, 41, 5, 38, 40, 25, 4, 30, 5, 7, 37, 53, 48, 52, 42, 56, 1, 56, 39, 53, 36, 42, 28, 55, 29, 14, 13, 45, 6, 46, 46, 53, 5, 49, 24, 51, 55, 2, 40, 2, 53, 43, 58, 33, 56, 57, 21, 22, 12, 7, 29, 27, 25, 47, 54, 52, 1, 24, 17, 10, 49, 40, 55, 40, 51, 31, 7, 13, 38, 19, 25, 13, 4, 36, 12, 37, 48, 50, 57, 22, 53, 18, 41, 12, 20, 50, 23, 32, 5, 29, 36, 41, 19, 38, 10, 0, 24, 39, 11, 10, 50, 15, 34, 9, 0, 29, 43, 27, 24, 38, 48, 29, 23, 57, 8, 50, 1, 20, 39, 46, 36, 10, 59, 56, 38, 5, 23, 40, 24, 14, 27, 29, 13, 42, 25, 10, 46, 34, 0, 33, 52, 9, 57, 11, 46, 38, 9, 50, 39, 35, 10, 25, 25, 51, 3, 7, 24, 12, 1, 12, 46, 27, 27, 35, 21, 43, 45, 55, 49, 5, 4, 15, 3, 14, 27, 4, 47, 21, 5, 39, 13, 26, 23, 6, 46, 27, 8, 18, 9, 26, 53, 14, 39, 1, 57, 14, 55, 50, 45, 24, 56, 1, 9, 33, 27, 9, 58, 54, 47, 42, 49, 5, 57, 54, 45, 25, 51, 47, 26, 42, 3, 23, 34, 35, 57, 9, 17, 31, 27, 31, 55, 13, 52, 9, 1, 31, 43, 48, 6, 57, 1, 36, 25, 58, 15, 39, 56, 18, 33, 8, 58, 40, 15, 29, 17, 5, 50, 25, 0, 5, 25, 54, 45, 4, 7, 17, 43, 32, 0, 56, 55, 6, 4, 55, 49, 50, 11, 26, 55, 14, 55, 57, 41, 1, 11, 59, 11, 54, 8, 12, 9, 14, 9, 44, 27, 12, 12, 59, 0, 31, 35, 31, 22, 9, 47, 21, 34, 12, 44, 53, 2, 47, 16, 8, 40, 40, 3, 41, 12, 35, 21, 24, 10, 2, 9, 41, 17, 53, 20, 30, 19, 12, 55, 0, 25, 25, 56, 25, 7, 41, 55, 8, 22, 59, 52, 38, 13, 59, 30, 49, 37, 21, 52, 11, 22, 42, 9, 1, 10, 7, 21, 8, 15, 2, 14, 51, 34, 23, 45, 37, 54, 38, 8, 32, 39, 53, 20, 6, 50, 11, 57, 47, 28, 35, 26, 43, 50, 50, 19, 5, 35, 0, 55, 0, 11, 18, 9, 21, 21, 19, 23, 51, 46, 30, 39, 8, 12, 42, 38, 26, 27, 17, 1, 43, 44, 21, 1, 46, 58, 14, 4, 20, 31, 4, 40, 16, 21, 25, 12, 4, 39, 28, 48, 28, 25, 8, 12, 26, 0, 42, 42, 31, 23, 32, 49, 2, 16, 29, 1, 48, 26, 43, 23, 19, 42, 3, 30, 14, 5, 11, 26, 59, 58, 1, 14, 0, 53, 11, 38, 50, 35, 7, 13, 31, 16, 30, 53, 57, 55, 1, 5, 28, 41, 58, 57, 23, 21, 38, 2, 1, 17, 51, 51, 10, 51, 14, 53, 3, 4, 43, 51, 52, 51, 30, 36, 26, 20, 36, 22, 54, 31, 44, 3, 5, 58, 26, 32, 34, 58, 0, 54, 23, 6, 15, 32, 1, 26, 55, 0, 40, 20, 0, 13, 28, 32, 41, 59, 18, 29, 10, 23, 18, 51, 9, 45, 42, 18, 11, 12, 59, 42, 11, 49, 37, 9, 40, 51, 3, 4, 11, 11, 51, 21, 49, 36, 17, 0, 59, 50, 6, 59, 4, 51, 54, 39, 26, 47, 59, 18, 15, 52, 56, 55, 3, 58, 26, 42, 29, 17, 0, 54, 24, 55, 16, 3, 18, 41, 31, 1, 13, 54, 5, 9, 16, 50, 12, 43, 2, 49, 56, 43, 0, 55, 15, 50, 35, 38, 53, 59, 14, 49, 38, 9, 17, 58, 40, 3, 21, 34, 56, 44, 40, 21, 44, 50, 17, 47, 53, 7, 39, 30, 1, 57, 55, 22, 18, 11, 2, 53, 2, 57, 29, 24, 34, 51, 15, 10, 12, 36, 47, 32, 10, 13, 48, 30, 21, 30, 42, 26, 48, 46, 13, 2, 50, 16, 0, 56, 26, 44, 34, 53, 19, 56, 56, 24, 59, 34, 0, 40, 39, 23, 55, 13, 8, 8, 23, 54, 13, 57, 42, 3, 29, 30, 47, 14, 26, 4, 8, 7, 40, 32, 7, 8, 11, 1, 0, 18, 56, 39, 3, 25, 2, 35, 4, 41, 13, 25, 22, 15, 13, 30, 50, 57, 12, 10, 1, 4, 52, 2, 7, 10, 19, 23, 53, 8, 28, 3, 47, 38, 34, 51, 44, 25, 21, 21, 27, 55, 52, 50, 0, 57, 37, 46, 8, 45, 11, 41, 22, 9, 15, 53, 23, 44, 22, 54, 58, 52, 11, 20, 20, 21, 51, 4, 57, 12, 26, 56, 14, 37, 13, 55, 31, 58, 44, 21, 29, 12, 55, 14, 45, 57, 50, 39, 41, 13, 6, 36, 31, 26, 59, 17, 37, 13, 5, 55, 10, 30, 32, 33, 11, 27, 9, 50, 29, 7, 8, 58, 42, 3, 25, 24, 49, 4, 41, 54, 50, 50, 17, 25, 45, 16, 49, 53, 16, 54, 36, 10, 14, 17, 0, 9, 6, 37, 37, 52, 25, 30, 7, 10, 6, 23, 53, 55, 52, 29, 44, 5, 47, 10, 17, 48, 7, 31, 55, 5, 21, 14, 12, 28, 11, 11, 22, 12, 0, 30, 44, 52, 51, 51, 38, 55, 9, 17, 49, 3, 44, 11, 32, 29, 37, 10, 51, 9, 2, 0, 54, 58, 24, 7, 59, 16, 53, 23, 41, 47, 36, 26, 58, 54, 2, 49, 47, 23, 47, 48, 46, 7, 8, 26, 55, 39, 8, 15, 33, 54, 45, 22, 43, 3, 50, 1, 21, 44, 57, 43, 33, 40, 42, 20, 31, 33, 32, 7, 22, 3, 59, 19, 12, 17, 18, 59, 54, 4, 34, 15, 53, 40, 15, 23, 58, 7, 47, 27, 50, 13, 51, 48, 58, 1, 22, 29, 26, 31, 49, 58, 49, 23, 33, 58, 23, 18, 55, 3, 7, 42, 23, 22, 12, 37, 5, 14, 53, 48, 0, 43, 17, 45, 19, 8, 27, 34, 28, 58, 22, 47, 52, 8, 10, 15, 58, 56, 15, 20, 15, 58, 44, 14, 26, 8, 59, 10, 28, 7, 50, 26, 52, 11, 18, 23, 5, 42, 8, 47, 49, 35, 10, 50, 8, 59, 13, 34, 8, 33, 18, 3, 17, 14, 1, 22, 44, 38, 12, 56, 59, 56, 46, 7, 37, 14, 18, 53, 24, 47, 1, 53, 59, 8, 11, 13, 52, 8, 54, 54, 55, 27, 39, 25, 53, 46, 59, 9, 53, 13, 33, 40, 42, 52, 59, 40, 52, 47, 23, 53, 14, 45, 8, 43, 0, 10, 0, 0, 9, 30, 9, 3, 47, 25, 17, 36, 42, 53, 5, 33, 7, 45, 47, 19, 3, 6, 29, 52, 55, 10, 55, 49, 7, 2, 44, 2, 55, 33, 51, 33, 14, 50, 11, 49, 55, 15, 48, 50, 36, 22, 17, 51, 11, 7, 24, 5, 22, 23, 46, 37, 13, 13, 20, 29, 14, 55, 42, 49, 30, 40, 47, 19, 58, 25, 20, 36, 41, 30, 57, 58, 52, 48, 55, 33, 51, 7, 41, 18, 38, 45, 10, 15, 27, 27, 20, 40, 29, 18, 44, 1, 45, 50, 53, 46, 10, 5, 11, 45, 22, 11, 33, 1, 7, 40, 19, 0, 46, 32, 11, 34, 45, 15, 9, 55, 3, 41, 46, 9, 58, 34, 19, 42, 14, 9, 59, 56, 30, 4, 30, 22, 21, 39, 19, 34, 21, 37, 45, 49, 53, 13, 35, 10, 14, 56, 22, 3, 29, 50, 47, 59, 51, 9, 37, 5, 58, 48, 24, 33, 49, 29, 58, 48, 51, 31, 15, 33, 0, 48, 22, 25, 14, 34, 28, 45, 29, 30, 40, 35, 41, 18, 41, 11, 12, 7, 17, 32, 3, 12, 18, 3, 13, 54, 25, 43, 27, 59, 7, 13, 5, 54, 34, 1, 15, 51, 9, 57, 40, 47, 36, 58, 9, 50, 58, 58, 52, 21, 58, 56, 47, 5, 24, 18, 55, 12, 51, 37, 54, 10, 54, 31, 26, 56, 15, 41, 40, 23, 42, 7, 10, 57, 37, 15, 49, 35, 24, 41, 9, 56, 59, 9, 26, 20, 3, 53, 7, 56, 33, 10, 11, 4, 44, 5, 14, 57, 23, 13, 39, 43, 3, 34, 58, 29, 37, 55, 31, 56, 0, 3, 0, 54, 20, 16, 18, 8, 48, 21, 11, 33, 36, 56, 2, 1, 26, 49, 23, 59, 18, 11, 34, 55, 11, 53, 36, 46, 59, 22, 30, 53, 24, 45, 18, 26, 22, 8, 38, 0, 30, 40, 12, 20, 33, 15, 56, 3, 53, 52, 16, 52, 2, 30, 2, 36, 57, 16, 3, 47, 6, 10, 2, 1, 46, 37, 16, 24, 52, 7, 40, 19, 40, 26, 22, 11, 59, 29, 31, 57, 26, 59, 59, 18, 42, 46, 36, 10, 59, 14, 47, 37, 33, 45, 29, 29, 53, 53, 56, 7, 51, 17, 22, 44, 53, 32, 13, 17, 50, 36, 16, 34, 13, 20, 58, 49, 55, 51, 41, 49, 50, 51, 50, 6, 57, 6, 56, 25, 26, 53, 3, 46, 56, 53, 19, 47, 38, 35, 32, 39, 24, 7, 31, 29, 27, 49, 25, 52, 4, 43, 59, 58, 15, 23, 36, 52, 19, 33, 26, 6, 15, 5, 11, 17, 42, 39, 21, 41, 22, 4, 3, 27, 34, 24, 59, 38, 20, 49, 17, 22, 50, 5, 39, 57, 9, 47, 34, 1, 28, 56, 37, 54, 32, 38, 28, 10, 13, 39, 0, 51, 47, 53, 12, 52, 44, 42, 55, 15, 39, 0, 15, 54, 32, 42, 9, 51, 46, 0, 18, 59, 23, 18, 3, 42, 59, 37, 51, 54, 45, 43, 0, 18, 58, 1, 45, 41, 56, 22, 33, 51, 32, 38, 10, 56, 55, 16, 19, 17, 5, 57, 4, 26, 43, 46, 2, 32, 10, 27, 10, 36, 19, 32, 30, 57, 55, 36, 29, 4, 2, 5, 6, 42, 27, 47, 49, 22, 16, 16, 19, 15, 50, 5, 21, 51, 14, 7, 11, 22, 55, 1, 43, 47, 42, 28, 28, 33, 19, 49, 34, 13, 5, 51, 26, 18, 17, 46, 1, 54, 18, 2, 26, 22, 54, 48, 19, 56, 41, 10, 37, 35, 51, 54, 21, 28, 36, 28, 3, 16, 4, 18, 16, 32, 59, 1, 12, 19, 16, 8, 3, 51, 11, 8, 26, 32, 16, 50, 46, 32, 18, 57, 36, 43, 30, 19, 7, 43, 25, 50, 50, 58, 57, 45, 4, 34, 46, 15, 10, 6, 2, 6, 35, 7, 32, 39, 2, 28, 29, 26, 53, 40, 9, 16, 14, 30, 37, 38, 2, 15, 0, 5, 50, 20, 44, 59, 42, 57, 17, 45, 11, 14, 23, 21, 14, 17, 37, 32, 2, 23, 54, 57, 58, 37, 51, 27, 13, 16, 30, 8, 44, 20, 50, 54, 39, 11, 55, 55, 19, 0, 40, 49, 56, 53, 55, 13, 23, 38, 57, 45, 29, 39, 59, 20, 7, 12, 54, 11, 52, 41, 25, 20, 30, 8, 50, 6, 11, 27, 12, 22, 13, 33, 30, 7, 39, 10, 25, 55, 51, 51, 56, 54, 15, 10, 12, 28, 24, 35, 35, 56, 6, 20, 46, 27, 17, 8, 18, 44, 8, 31, 12, 9, 56, 14, 55, 25, 14, 4, 39, 11, 40, 56, 26, 7, 38, 56, 25, 2, 47, 43, 27, 9, 12, 21, 38, 10, 1, 9, 42, 12, 15, 41, 46, 34, 38, 29, 52, 54, 27, 51, 34, 37, 2, 58, 0, 56, 46, 4, 13, 54, 11, 51, 3, 34, 42, 14, 18, 9, 43, 38, 3, 17, 35, 50, 6, 4, 33, 44, 48, 16, 22, 13, 42, 9, 58, 6, 10, 57, 19, 8, 38, 0, 47, 48, 58, 23, 56, 51, 28, 11, 55, 19, 6, 0, 20, 17, 9, 4, 20, 12, 14, 11, 20, 53, 33, 54, 22, 36, 59, 9, 58, 5, 55, 0, 25, 14, 4, 1, 1, 55, 54, 13, 47, 31, 2, 38, 28, 28, 27, 43, 26, 32, 22, 2, 40, 58, 5, 4, 48, 36, 16, 42, 1, 3, 31, 58, 32, 0, 7, 45, 5, 25, 10, 10, 23, 1, 3, 49, 33, 16, 55, 55, 1, 50, 36, 6, 20, 20, 5, 16, 21, 58, 42, 19, 7, 27, 31, 15, 8, 31, 51, 30, 57, 51, 59, 45, 49, 16, 5, 51, 7, 52, 29, 13, 58, 36, 16, 37, 39, 2, 24, 59, 6, 40, 21, 21, 8, 57, 7, 26, 35, 45, 58, 45, 34, 32, 4, 24, 57, 18, 56, 8, 23, 32, 12, 32, 58, 1, 27, 9, 38, 23, 1, 56, 59, 9, 19, 14, 8, 7, 6, 6, 51, 25, 59, 32, 16, 54, 39, 10, 7, 22, 6, 30, 2, 6, 42, 28, 33, 41, 23, 26, 49, 49, 32, 7, 33, 35, 58, 56, 28, 44, 14, 33, 44, 36, 40, 59, 38, 12, 5, 5, 40, 0, 21, 48, 28, 40, 25, 26, 26, 12, 36, 59, 7, 53, 33, 59, 5, 51, 26, 27, 48, 50, 49, 17, 14, 31, 39, 5, 53, 21, 27, 56, 44, 54, 0, 16, 10, 9, 19, 12, 16, 18, 53, 56, 14, 42, 56, 58, 7, 12, 56, 1, 43, 14, 57, 44, 36, 15, 27, 41, 28, 6, 2, 3, 30, 48, 27, 4, 15, 28, 50, 32, 19, 23, 57, 10, 22, 27, 42, 26, 29, 1, 18, 50, 30, 8, 36, 26, 51, 20, 54, 50, 39, 59, 19, 26, 56, 4, 2, 0, 6, 36, 42, 32, 11, 38, 53, 1, 31, 23, 30, 51, 19, 10, 33, 52, 9, 24, 15, 0, 20, 38, 5, 56, 47, 6, 19, 3, 9, 25, 27, 49, 43, 24, 58, 48, 43, 0, 6, 27, 53, 48, 25, 46, 12, 57, 47, 42, 5, 37, 55, 53, 20, 27, 20, 48, 7, 34, 13, 58, 20, 36, 35, 31, 13, 11, 18, 23, 36, 40, 51, 3, 56, 25, 25, 42, 57, 55, 27, 12, 20, 38, 30, 54, 24, 54, 52, 0, 42, 53, 14, 39, 57, 50, 2, 24, 8, 15, 39, 6, 5, 59, 16, 46, 49, 26, 42, 15, 14, 10, 2, 52, 49, 56, 19, 25, 25, 1, 36, 25, 15, 10, 22, 13, 52, 40, 50, 29, 55, 6, 32, 28, 41, 56, 55, 25, 39, 34, 18, 5, 12, 26, 25, 58, 35, 41, 22, 11, 4, 59, 56, 0, 30, 55, 2, 50, 28, 59, 23, 8, 56, 24, 51, 37, 2, 47, 20, 24, 35, 6, 52, 53, 24, 49, 17, 19, 53, 59, 23, 39, 3, 56, 31, 54, 15, 24, 26, 18, 30, 4, 13, 14, 33, 10, 0, 7, 41, 58, 44, 51, 47, 41, 14, 24, 40, 34, 45, 50, 56, 0, 46, 35, 22, 42, 59, 33, 11, 39, 31, 22, 54, 35, 1, 16, 1, 32, 26, 32, 35, 24, 31, 15, 43, 24, 21, 1, 58, 8, 13, 6, 0, 58, 7, 59, 45, 40, 56, 8, 19, 41, 48, 57, 4, 7, 48, 58, 4, 21, 53, 59, 6, 23, 27, 57, 50, 3, 39, 59, 20, 9, 47, 13, 42, 26, 18, 42, 47, 28, 16, 4, 53, 49, 24, 19, 33, 25, 39, 27, 34, 27, 50, 12, 1, 2, 7, 56, 58, 49, 21, 37, 51, 27, 59, 48, 59, 13, 14, 57, 5, 2, 55, 6, 25, 45, 34, 21, 12, 57, 58, 39, 9, 57, 23, 0, 29, 49, 28, 49, 16, 20, 2, 7, 16, 14, 42, 24, 51, 11, 19, 28, 13, 40, 56, 50, 49, 3, 22, 22, 56, 20, 53, 6, 11, 33, 30, 2, 47, 56, 26, 21, 0, 9, 21, 55, 46, 41, 0, 37, 35, 34, 29, 1, 20, 13, 12, 24, 19, 38, 3, 36, 59, 41, 59, 58, 59, 53, 39, 10, 42, 44, 4, 19, 18, 56, 27, 0, 54, 51, 59, 5, 23, 54, 59, 58, 58, 2, 21, 58, 29, 57, 21, 38, 35, 40, 28, 39, 15, 4, 30, 53, 58, 9, 54, 54, 41, 6, 39, 25, 44, 36, 43, 17, 17, 39, 55, 3, 48, 34, 29, 31, 58, 5, 4, 3, 57, 12, 47, 6, 9, 38, 54, 46, 6, 41, 4, 23, 52, 8, 31, 26, 24, 55, 32, 40, 7, 26, 28, 1, 19, 59, 58, 5, 32, 29, 43, 45, 11, 33, 12, 19, 10, 53, 29, 27, 47, 43, 48, 6, 18, 58, 29, 55, 15, 36, 42, 58, 53, 56, 53, 58, 58, 31, 51, 59, 48, 28, 54, 26, 57, 46, 24, 39, 35, 19, 12, 55, 59, 41, 17, 27, 1, 49, 7, 34, 6, 6, 29, 6, 26, 57, 58, 57, 50, 13, 36, 1, 5, 0, 9, 43, 45, 57, 9, 24, 0, 52, 5, 5, 30, 57, 2, 11, 21, 10, 23, 17, 18, 36, 39, 3, 20, 57, 39, 41, 13, 55, 15, 50, 47, 2, 31, 11, 10, 48, 18, 53, 57, 33, 11, 12, 53, 20, 21, 12, 14, 25, 6, 57, 25, 27, 7, 22, 9, 15, 51, 9, 14, 28, 27, 18, 0, 16, 36, 6, 21, 25, 33, 58, 52, 56, 21, 56, 35, 35, 11, 3, 17, 41, 3, 9, 56, 51, 32, 11, 20, 10, 15, 10, 5, 49, 50, 59, 40, 0, 17, 48, 43, 25, 42, 16, 54, 52, 16, 17, 54, 14, 34, 28, 38, 51, 57, 14, 23, 37, 58, 17, 33, 59, 40, 3, 54, 44, 32, 7, 56, 49, 48, 10, 11, 55, 51, 52, 51, 3, 45, 11, 16, 0, 59, 53, 41, 50, 8, 47, 16, 27, 37, 27, 58, 21, 21, 31, 31, 52, 50, 7, 37, 39, 44, 44, 25, 58, 1, 6, 49, 0, 54, 5, 17, 37, 15, 19, 30, 36, 9, 51, 11, 25, 37, 17, 52, 51, 24, 14, 36, 51, 54, 54, 40, 2, 53, 58, 10, 51, 24, 58, 46, 44, 3, 25, 45, 16, 5, 18, 54, 37, 51, 42, 46, 17, 23, 2, 34, 19, 53, 54, 26, 46, 57, 41, 10, 5, 57, 52, 6, 51, 33, 27, 31, 30, 6, 0, 25, 10, 15, 20, 26, 11, 7, 53, 45, 52, 38, 50, 44, 49, 50, 41, 24, 52, 16, 6, 32, 15, 52, 32, 8, 0, 6, 4, 4, 57, 17, 34, 58, 4, 55, 52, 42, 15, 16, 6, 4, 25, 45, 9, 7, 58, 18, 21, 13, 44, 56, 19, 23, 58, 8, 55, 19, 57, 31, 54, 36, 55, 0, 46, 50, 8, 50, 58, 53, 54, 45, 11, 42, 21, 48, 2, 33, 16, 6, 18, 8, 24, 59, 45, 26, 59, 29, 22, 31, 7, 56, 54, 14, 13, 20, 7, 41, 52, 1, 49, 54, 17, 2, 32, 16, 28, 0, 59, 50, 51, 43, 18, 17, 27, 31, 12, 59, 29, 8, 8, 52, 48, 45, 20, 23, 43, 28, 19, 38, 56, 9, 22, 24, 11, 5, 31, 39, 52, 2, 28, 5, 53, 30, 10, 37, 1, 58, 24, 5, 20, 10, 38, 22, 9, 12, 35, 14, 0, 47, 58, 26, 11, 25, 40, 58, 22, 53, 58, 52, 10, 47, 28, 55, 46, 52, 32, 1, 46, 18, 49, 51, 13, 18, 1, 13, 2, 22, 54, 15, 19, 55, 1, 21, 4, 21, 4, 5, 11, 5, 16, 23, 17, 5, 27, 43, 19, 52, 7, 5, 26, 53, 10, 56, 33, 45, 22, 1, 58, 39, 28, 34, 12, 9, 29, 11, 17, 44, 39, 56, 59, 9, 8, 36, 47, 28, 54, 25, 18, 19, 8, 56, 7, 22, 17, 40, 18, 13, 55, 17, 15, 50, 59, 33, 17, 42, 59, 40, 12, 48, 35, 3, 21, 7, 1, 58, 47, 2, 16, 52, 20, 6, 29, 4, 44, 47, 55, 35, 49, 36, 46, 45, 40, 13, 41, 18, 23, 54, 9, 43, 13, 14, 7, 12, 40, 35, 59, 16, 8, 14, 8, 22, 53, 3, 56, 39, 25, 12, 1, 37, 34, 46, 46, 46, 33, 58, 27, 43, 16, 16, 6, 39, 57, 8, 43, 50, 29, 49, 22, 55, 51, 55, 51, 2, 26, 5, 11, 40, 53, 38, 43, 17, 22, 12, 10, 49, 52, 48, 23, 9, 46, 50, 26, 5, 27, 36, 12, 52, 2, 46, 5, 13, 57, 59, 28, 52, 31, 56, 8, 11, 38, 12, 32, 44, 10, 10, 53, 38, 59, 55, 45, 35, 35, 51, 58, 57, 23, 52, 33, 15, 51, 12, 16, 7, 27, 7, 20, 23, 20, 3, 17, 40, 54, 11, 51, 23, 0, 26, 2, 41, 53, 58, 39, 41, 34, 35, 3, 2, 57, 26, 7, 25, 51, 30, 50, 39, 53, 14, 38, 11, 3, 8, 45, 45, 30, 51, 26, 1, 4, 16, 35, 47, 42, 10, 8, 12, 37, 15, 28, 33, 51, 0, 57, 31, 25, 7, 20, 45, 57, 48, 38, 9, 51, 57, 33, 4, 16, 50, 20, 16, 35, 51, 11, 57, 54, 59, 38, 48, 20, 27, 35, 4, 9, 33, 15, 56, 15, 6, 54, 23, 23, 20, 22, 31, 7, 45, 59, 26, 42, 28, 3, 4, 9, 46, 59, 4, 44, 9, 54, 53, 21, 55, 40, 46, 18, 30, 10, 24, 51, 16, 15, 29, 13, 3, 8, 25, 57, 15, 30, 6, 5, 1, 43, 26, 34, 26, 5, 34, 59, 21, 54, 1, 50, 22, 16, 53, 17, 54, 55, 56, 53, 18, 57, 51, 52, 38, 8, 29, 18, 45, 40, 5, 28, 52, 15, 54, 2, 15, 38, 16, 37, 27, 54, 27, 30, 6, 45, 58, 52, 6, 40, 29, 16, 53, 54, 53, 56, 23, 18, 50, 53, 2, 26, 15, 18, 24, 14, 18, 25, 55, 28, 23, 8, 2, 19, 23, 31, 40, 59, 28, 52, 7, 3, 48, 11, 21, 7, 19, 48, 35, 34, 21, 43, 9, 33, 5, 7, 27, 27, 53, 15, 10, 59, 19, 11, 58, 1, 51, 56, 0, 54, 5, 40, 40, 49, 27, 5, 19, 25, 36, 6, 59, 20, 12, 22, 47, 29, 8, 51, 5, 1, 39, 0, 58, 45, 10, 19, 20, 46, 30, 58, 56, 34, 6, 12, 51, 2, 57, 22, 36, 37, 53, 59, 29, 51, 55, 47, 24, 24, 58, 59, 47, 58, 24, 31, 2, 40, 25, 58, 15, 55, 47, 39, 1, 54, 53, 50, 33, 43, 50, 26, 1, 53, 2, 40, 46, 3, 36, 21, 29, 56, 58, 1, 41, 38, 21, 55, 55, 38, 50, 58, 10, 5, 31, 49, 29, 10, 24, 49, 55, 17, 53, 32, 18, 15, 18, 48, 42, 59, 53, 32, 40, 27, 42, 58, 3, 50, 20, 47, 53, 54, 14, 17, 24, 42, 27, 50, 18, 45, 13, 7, 32, 4, 46, 52, 2, 46, 27, 10, 48, 53, 11, 20, 50, 59, 38, 36, 59, 5, 56, 29, 11, 22, 26, 41, 30, 7, 43, 7, 59, 58, 7, 41, 41, 55, 19, 7, 50, 15, 44, 37, 47, 34, 59, 40, 33, 55, 14, 4, 49, 19, 1, 26, 5, 1, 8, 10, 56, 23, 55, 40, 52, 32, 1, 50, 19, 58, 45, 25, 39, 2, 11, 28, 53, 56, 57, 28, 43, 2, 47, 9, 53, 39, 29, 51, 2, 24, 1, 10, 15, 51, 57, 6, 5, 28, 20, 19, 1, 24, 26, 53, 38, 9, 39, 0, 31, 21, 32, 10, 3, 41, 47, 27, 38, 42, 59, 54, 5, 40, 56, 12, 28, 50, 11, 22, 12, 52, 54, 1, 38, 17, 25, 52, 4, 49, 40, 45, 25, 49, 8, 17, 48, 9, 57, 32, 32, 35, 50, 50, 45, 11, 12, 38, 24, 35, 16, 59, 1, 55, 32, 41, 51, 25, 23, 16, 49, 58, 36, 15, 59, 49, 42, 27, 13, 59, 27, 52, 26, 7, 44, 24, 28, 28, 7, 17, 7, 5, 52, 43, 21, 20, 15, 51, 22, 0, 9, 2, 2, 21, 5, 10, 13, 26, 57, 18, 58, 18, 28, 53, 52, 2, 36, 46, 11, 31, 36, 30, 16, 30, 44, 29, 27, 59, 42, 11, 38, 8, 1, 39, 36, 13, 33, 53, 58, 21, 14, 7, 57, 54, 22, 4, 26, 7, 55, 12, 19, 14, 0, 17, 12, 41, 33, 34, 49, 56, 13, 11, 8, 11, 26, 27, 37, 24, 55, 15, 42, 23, 26, 59, 38, 55, 41, 9, 0, 47, 37, 36, 27, 14, 14, 0, 44, 44, 13, 5, 22, 0, 14, 49, 26, 36, 8, 23, 35, 1, 1, 5, 3, 12, 23, 9, 16, 13, 10, 39, 43, 41, 31, 20, 41, 53, 0, 16, 46, 59, 47, 11, 6, 9, 14, 49, 20, 10, 11, 35, 20, 44, 40, 33, 30, 28, 52, 25, 19, 2, 48, 58, 59, 33, 47, 44, 33, 0, 47, 58, 50, 15, 32, 59, 15, 59, 6, 36, 35, 1, 56, 55, 31, 13, 15, 34, 7, 8, 11, 19, 19, 29, 2, 18, 8, 52, 51, 0, 11, 51, 26, 45, 58, 45, 53, 56, 17, 3, 40, 26, 1, 42, 54, 13, 38, 50, 8, 22, 25, 42, 42, 21, 15, 41, 22, 30, 5, 22, 33, 52, 4, 27, 22, 52, 11, 13, 35, 42, 0, 21, 32, 8, 54, 3, 24, 49, 55, 43, 45, 11, 9, 17, 38, 56, 52, 34, 1, 59, 28, 36, 53, 19, 29, 35, 35, 2, 43, 45, 12, 58, 19, 53, 30, 40, 7, 14, 15, 31, 30, 29, 39, 56, 36, 1, 32, 21, 3, 57, 56, 32, 56, 53, 18, 8, 5, 26, 22, 57, 44, 30, 22, 21, 16, 7, 46, 39, 37, 51, 27, 26, 36, 24, 11, 36, 1, 30, 56, 37, 18, 20, 35, 22, 56, 11, 17, 7, 10, 18, 16, 53, 54, 34, 57, 24, 26, 8, 9, 3, 58, 1, 18, 20, 18, 6, 35, 59, 57, 19, 2, 42, 42, 21, 10, 37, 28, 19, 10, 6, 49, 16, 23, 49, 29, 35, 22, 23, 6, 46, 57, 21, 56, 57, 30, 34, 24, 31, 23, 14, 11, 56, 30, 34, 45, 55, 43, 18, 50, 14, 9, 13, 13, 44, 45, 32, 49, 56, 1, 58, 26, 27, 26, 4, 36, 9, 26, 46, 14, 34, 17, 31, 46, 57, 26, 23, 18, 58, 20, 51, 13, 1, 53, 22, 35, 30, 53, 26, 24, 12, 15, 13, 31, 27, 34, 7, 52, 20, 35, 41, 32, 36, 38, 0, 55, 13, 24, 30, 27, 0, 46, 53, 49, 24, 16, 57, 14, 30, 13, 28, 14, 19, 28, 45, 11, 17, 14, 48, 51, 33, 16, 14, 39, 41, 10, 9, 18, 15, 51, 57, 9, 48, 46, 42, 10, 30, 14, 29, 55, 41, 10, 4, 44, 54, 45, 13, 45, 39, 55, 7, 3, 12, 26, 39, 57, 27, 0, 35, 6, 43, 46, 0, 38, 45, 1, 23, 19, 13, 8, 9, 8, 5, 42, 26, 10, 11, 3, 4, 1, 25, 6, 45, 47, 24, 49, 34, 27, 43, 22, 3, 17, 26, 23, 2, 11, 27, 9, 30, 47, 4, 0, 45, 47, 9, 13, 3, 50, 44, 27, 29, 31, 19, 9, 55, 24, 25, 51, 32, 48, 50, 38, 31, 1, 15, 11, 53, 16, 52, 12, 19, 43, 6, 43, 45, 55, 59, 56, 18, 42, 24, 40, 13, 17, 39, 33, 51, 56, 56, 22, 45, 1, 24, 7, 33, 56, 26, 32, 46, 6, 48, 18, 5, 22, 57, 3, 45, 59, 45, 6, 20, 50, 47, 0, 32, 41, 9, 57, 15, 47, 4, 53, 3, 50, 2, 54, 28, 18, 48, 56, 10, 9, 19, 42, 9, 21, 1, 11, 41, 53, 17, 16, 6, 45, 35, 35, 15, 45, 14, 23, 58, 15, 22, 15, 33, 44, 40, 18, 3, 24, 57, 58, 47, 36, 42, 37, 8, 20, 2, 18, 51, 39, 8, 7, 7, 1, 45, 36, 28, 21, 21, 11, 27, 3, 20, 30, 47, 27, 4, 3, 16, 18, 5, 15, 13, 55, 26, 28, 3, 59, 19, 59, 22, 58, 23, 12, 54, 26, 48, 34, 16, 15, 57, 10, 6, 42, 18, 2, 42, 27, 39, 2, 18, 5, 11, 9, 13, 58, 21, 56, 8, 12, 31, 37, 54, 3, 21, 20, 43, 57, 44, 0, 2, 58, 9, 21, 27, 41, 51, 58, 12, 4, 54, 57, 46, 48, 6, 19, 16, 45, 11, 6, 36, 38, 44, 21, 47, 21, 27, 5, 37, 46, 24, 56, 14, 44, 13, 35, 16, 6, 8, 45, 11, 58, 59, 9, 59, 14, 4, 13, 33, 11, 50, 22, 22, 22, 22, 32, 15, 14, 21, 38, 36, 8, 37, 27, 5, 52, 54, 40, 42, 22, 50, 44, 3, 53, 24, 58, 29, 24, 19, 15, 9, 56, 9, 44, 22, 11, 1, 4, 56, 47, 23, 30, 12, 30, 16, 17, 6, 24, 37, 28, 19, 59, 1, 32, 14, 27, 57, 9, 51, 28, 46, 16, 2, 19, 50, 47, 21, 38, 8, 57, 27, 30, 57, 5, 19, 54, 15, 46, 24, 8, 35, 39, 17, 43, 56, 6, 21, 28, 14, 41, 36, 54, 32, 52, 50, 55, 17, 2, 24, 39, 59, 4, 7, 14, 23, 19, 53, 25, 49, 55, 2, 59, 51, 52, 6, 56, 54, 6, 5, 49, 23, 37, 36, 50, 2, 46, 40, 23, 53, 10, 23, 49, 27, 16, 13, 24, 12, 55, 42, 52, 56, 21, 1, 25, 30, 11, 39, 29, 2, 33, 32, 4, 53, 26, 48, 3, 32, 32, 48, 54, 19, 44, 42, 58, 58, 57, 54, 17, 31, 14, 29, 29, 26, 28, 48, 33, 57, 7, 54, 38, 19, 58, 15, 21, 46, 36, 35, 51, 6, 15, 49, 18, 4, 56, 27, 24, 22, 43, 0, 21, 58, 27, 55, 56, 16, 27, 2, 57, 48, 2, 9, 12, 47, 57, 36, 1, 43, 58, 41, 16, 41, 7, 39, 1, 56, 41, 20, 23, 59, 46, 36, 48, 54, 49, 1, 7, 55, 15, 59, 38, 14, 52, 25, 56, 23, 55, 33, 43, 54, 14, 53, 50, 51, 8, 54, 21, 57, 28, 27, 57, 42, 27, 46, 30, 58, 11, 14, 13, 0, 16, 47, 27, 27, 4, 10, 18, 15, 34, 44, 57, 26, 22, 1, 20, 17, 36, 34, 29, 33, 56, 1, 17, 54, 4, 8, 4, 48, 55, 51, 0, 36, 28, 39, 51, 46, 25, 24, 41, 10, 48, 53, 16, 54, 42, 30, 54, 8, 2, 20, 30, 58, 50, 28, 26, 46, 50, 20, 0, 25, 3, 57, 48, 37, 0, 13, 57, 24, 52, 53, 40, 26, 13, 14, 54, 12, 56, 17, 10, 46, 11, 53, 19, 58, 2, 3, 38, 52, 19, 13, 53, 28, 16, 48, 17, 59, 53, 23, 26, 22, 17, 20, 7, 4, 4, 6, 22, 10, 28, 40, 56, 53, 25, 55, 24, 18, 23, 1, 15, 26, 12, 55, 21, 13, 17, 20, 6, 24, 56, 7, 9, 10, 13, 54, 8, 54, 51, 47, 21, 35, 4, 9, 35, 4, 55, 58, 29, 36, 26, 21, 15, 36, 0, 41, 56, 55, 27, 18, 57, 53, 10, 10, 48, 29, 58, 23, 13, 8, 32, 15, 54, 57, 24, 35, 54, 21, 3, 49, 23, 25, 32, 56, 53, 48, 17, 23, 30, 33, 48, 3, 12, 17, 36, 45, 8, 36, 54, 11, 47, 8, 50, 47, 30, 22, 29, 58, 42, 44, 50, 50, 28, 59, 31, 15, 24, 47, 35, 25, 54, 18, 50, 41, 31, 37, 1, 57, 6, 18, 14, 12, 57, 59, 2, 14, 15, 5, 19, 38, 26, 57, 26, 11, 24, 23, 40, 48, 8, 7, 23, 57, 57, 34, 43, 22, 47, 7, 3, 19, 59, 18, 9, 58, 19, 8, 48, 54, 57, 48, 51, 5, 41, 52, 28, 47, 39, 15, 40, 54, 36, 32, 44, 13, 45, 7, 49, 27, 2, 22, 32, 24, 23, 18, 18, 56, 57, 53, 40, 17, 59, 29, 38, 58, 4, 42, 49, 40, 16, 54, 25, 34, 17, 46, 17, 14, 23, 7, 26, 40, 37, 57, 9, 37, 16, 17, 18, 38, 51, 47, 14, 46, 52, 56, 8, 17, 20, 24, 48, 29, 50, 16, 11, 24, 0, 21, 4, 38, 52, 16, 45, 19, 29, 56, 6, 35, 16, 54, 54, 25, 3, 55, 46, 40, 49, 17, 16, 6, 48, 41, 2, 53, 52, 58, 21, 26, 7, 8, 32, 33, 23, 23, 53, 55, 46, 13, 2, 9, 1, 45, 21, 37, 1, 30, 5, 50, 19, 56, 10, 55, 7, 29, 51, 21, 20, 51, 45, 57, 37, 56, 59, 41, 6, 22, 37, 34, 57, 26, 15, 50, 52, 16, 11, 12, 10, 54, 48, 59, 23, 53, 51, 38, 10, 13, 49, 49, 35, 34, 5, 36, 59, 26, 2, 52, 31, 23, 1, 52, 14, 21, 4, 15, 42, 10, 56, 11, 17, 27, 22, 0, 51, 49, 3, 58, 59, 49, 23, 11, 54, 54, 7, 53, 34, 58, 33, 8, 4, 13, 31, 27, 15, 1, 22, 16, 30, 4, 4, 6, 22, 15, 38, 4, 42, 52, 56, 0, 6, 38, 10, 24, 51, 25, 55, 12, 34, 23, 9, 6, 25, 51, 53, 3, 31, 38, 16, 42, 50, 16, 59, 12, 21, 9, 7, 47, 5, 7, 45, 8, 52, 21, 40, 37, 30, 8, 56, 26, 2, 26, 54, 31, 55, 50, 29, 14, 14, 5, 55, 19, 53, 17, 36, 54, 53, 38, 21, 17, 19, 59, 24, 22, 21, 49, 4, 35, 2, 1, 15, 28, 0, 29, 48, 50, 26, 39, 7, 0, 13, 58, 54, 45, 14, 8, 58, 6, 3, 37, 41, 59, 25, 46, 36, 27, 55, 20, 25, 50, 55, 54, 17, 20, 43, 55, 31, 58, 44, 40, 55, 59, 9, 47, 2, 10, 54, 24, 55, 39, 54, 37, 12, 18, 25, 26, 19, 51, 36, 24, 26, 11, 25, 29, 47, 33, 9, 4, 12, 24, 27, 55, 16, 50, 18, 48, 36, 54, 5, 39, 28, 9, 2, 3, 0, 37, 57, 8, 27, 1, 38, 13, 45, 58, 49, 45, 14, 45, 16, 51, 55, 42, 15, 32, 5, 11, 50, 51, 24, 59, 27, 20, 31, 41, 4, 16, 1, 43, 0, 35, 36, 30, 24, 8, 28, 5, 54, 7, 26, 9, 56, 50, 52, 19, 55, 44, 25, 8, 8, 25, 34, 47, 53, 2, 30, 49, 44, 54, 7, 1, 20, 22, 17, 52, 55, 32, 46, 5, 2, 49, 1, 47, 17, 37, 27, 20, 53, 13, 47, 24, 24, 53, 19, 55, 59, 17, 13, 38, 7, 18, 14, 56, 14, 54, 10, 31, 52, 12, 13, 57, 37, 6, 55, 10, 15, 20, 8, 50, 1, 48, 35, 46, 57, 3, 55, 5, 32, 14, 19, 56, 24, 16, 25, 44, 1, 12, 18, 35, 29, 52, 3, 44, 3, 57, 54, 22, 47, 58, 55, 47, 56, 33, 43, 6, 13, 55, 55, 17, 54, 48, 19, 25, 35, 41, 52, 22, 56, 20, 4, 50, 18, 59, 9, 27, 0, 2, 19, 16, 25, 55, 12, 5, 52, 28, 41, 23, 54, 19, 1, 19, 58, 41, 50, 9, 30, 32, 50, 30, 17, 14, 46, 15, 58, 28, 23, 51, 17, 49, 9, 8, 18, 0, 7, 37, 47, 41, 18, 7, 27, 51, 23, 18, 51, 14, 27, 12, 0, 43, 18, 30, 5, 56, 16, 54, 51, 34, 51, 37, 5, 51, 2, 36, 21, 4, 41, 3, 52, 57, 56, 58, 46, 7, 3, 56, 29, 0, 58, 27, 31, 13, 51, 22, 1, 37, 33, 5, 41, 8, 28, 1, 7, 26, 56, 45, 19, 10, 21, 59, 5, 28, 54, 3, 24, 34, 55, 50, 27, 34, 49, 14, 4, 41, 56, 40, 33, 36, 1, 18, 56, 51, 0, 52, 42, 55, 46, 34, 14, 6, 47]\n",
      "8031\n",
      "Train shape:  (8031, 164, 225) (8031, 60) Test Shape:  (1276, 164, 225) (1276, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 11:46:21.338037: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-12-30 11:46:21.338055: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: DeepLearning2\n",
      "2023-12-30 11:46:21.338057: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: DeepLearning2\n",
      "2023-12-30 11:46:21.338144: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 525.147.5\n",
      "2023-12-30 11:46:21.338152: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 470.223.2\n",
      "2023-12-30 11:46:21.338154: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:244] kernel version 470.223.2 does not match DSO version 525.147.5 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------fold 1 -----------\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1703915189.086681  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/226 [============================>.] - ETA: 0s - loss: 3.6965 - accuracy: 0.1078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703915198.394094  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 12s 41ms/step - loss: 3.6947 - accuracy: 0.1082 - val_loss: 3.1184 - val_accuracy: 0.2450\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 2.9340 - accuracy: 0.2626 - val_loss: 2.5115 - val_accuracy: 0.4142\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 2.4916 - accuracy: 0.3722 - val_loss: 2.0910 - val_accuracy: 0.5647\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 2.1918 - accuracy: 0.4345 - val_loss: 1.7781 - val_accuracy: 0.6194\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.9417 - accuracy: 0.5046 - val_loss: 1.5683 - val_accuracy: 0.7052\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.7504 - accuracy: 0.5438 - val_loss: 1.3916 - val_accuracy: 0.7114\n",
      "Epoch 7/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.6255 - accuracy: 0.5809 - val_loss: 1.2723 - val_accuracy: 0.7463\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.4902 - accuracy: 0.6097 - val_loss: 1.1422 - val_accuracy: 0.7786\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.3780 - accuracy: 0.6423 - val_loss: 1.0147 - val_accuracy: 0.7960\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.2664 - accuracy: 0.6665 - val_loss: 0.9204 - val_accuracy: 0.8246\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.2041 - accuracy: 0.6816 - val_loss: 0.8153 - val_accuracy: 0.8520\n",
      "Epoch 12/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.1007 - accuracy: 0.7155 - val_loss: 0.7776 - val_accuracy: 0.8532\n",
      "Epoch 13/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.0326 - accuracy: 0.7240 - val_loss: 0.7281 - val_accuracy: 0.8694\n",
      "Epoch 14/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.9885 - accuracy: 0.7417 - val_loss: 0.7027 - val_accuracy: 0.8706\n",
      "Epoch 15/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.9168 - accuracy: 0.7527 - val_loss: 0.6164 - val_accuracy: 0.8980\n",
      "Epoch 16/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.8811 - accuracy: 0.7635 - val_loss: 0.5898 - val_accuracy: 0.8893\n",
      "Epoch 17/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.8328 - accuracy: 0.7757 - val_loss: 0.5483 - val_accuracy: 0.8993\n",
      "Epoch 18/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.8022 - accuracy: 0.7869 - val_loss: 0.5050 - val_accuracy: 0.9154\n",
      "Epoch 19/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.7655 - accuracy: 0.7963 - val_loss: 0.4889 - val_accuracy: 0.9017\n",
      "Epoch 20/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.7306 - accuracy: 0.8049 - val_loss: 0.4759 - val_accuracy: 0.9042\n",
      "Epoch 21/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6924 - accuracy: 0.8150 - val_loss: 0.4448 - val_accuracy: 0.9129\n",
      "Epoch 22/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6704 - accuracy: 0.8252 - val_loss: 0.4207 - val_accuracy: 0.9229\n",
      "Epoch 23/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6259 - accuracy: 0.8389 - val_loss: 0.3891 - val_accuracy: 0.9341\n",
      "Epoch 24/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6098 - accuracy: 0.8398 - val_loss: 0.3615 - val_accuracy: 0.9254\n",
      "Epoch 25/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5896 - accuracy: 0.8432 - val_loss: 0.3936 - val_accuracy: 0.9254\n",
      "Epoch 26/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5543 - accuracy: 0.8602 - val_loss: 0.3465 - val_accuracy: 0.9254\n",
      "Epoch 27/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5325 - accuracy: 0.8594 - val_loss: 0.3355 - val_accuracy: 0.9316\n",
      "Epoch 28/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5378 - accuracy: 0.8602 - val_loss: 0.3357 - val_accuracy: 0.9341\n",
      "Epoch 29/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5062 - accuracy: 0.8694 - val_loss: 0.3262 - val_accuracy: 0.9378\n",
      "Epoch 30/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4973 - accuracy: 0.8679 - val_loss: 0.3006 - val_accuracy: 0.9391\n",
      "Epoch 31/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4616 - accuracy: 0.8792 - val_loss: 0.2809 - val_accuracy: 0.9440\n",
      "Epoch 32/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4428 - accuracy: 0.8863 - val_loss: 0.2713 - val_accuracy: 0.9403\n",
      "Epoch 33/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4286 - accuracy: 0.8883 - val_loss: 0.2762 - val_accuracy: 0.9415\n",
      "Epoch 34/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4252 - accuracy: 0.8889 - val_loss: 0.2618 - val_accuracy: 0.9428\n",
      "Epoch 35/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4079 - accuracy: 0.8961 - val_loss: 0.2556 - val_accuracy: 0.9527\n",
      "Epoch 36/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4078 - accuracy: 0.8929 - val_loss: 0.2724 - val_accuracy: 0.9378\n",
      "Epoch 37/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3990 - accuracy: 0.8968 - val_loss: 0.2356 - val_accuracy: 0.9565\n",
      "Epoch 38/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3753 - accuracy: 0.8950 - val_loss: 0.2264 - val_accuracy: 0.9577\n",
      "Epoch 39/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3659 - accuracy: 0.9026 - val_loss: 0.2371 - val_accuracy: 0.9515\n",
      "Epoch 40/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3553 - accuracy: 0.9031 - val_loss: 0.2347 - val_accuracy: 0.9453\n",
      "Epoch 41/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3524 - accuracy: 0.9067 - val_loss: 0.2177 - val_accuracy: 0.9540\n",
      "Epoch 42/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3368 - accuracy: 0.9123 - val_loss: 0.1991 - val_accuracy: 0.9602\n",
      "Epoch 43/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3077 - accuracy: 0.9207 - val_loss: 0.2130 - val_accuracy: 0.9515\n",
      "Epoch 44/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3048 - accuracy: 0.9222 - val_loss: 0.1902 - val_accuracy: 0.9614\n",
      "Epoch 45/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3204 - accuracy: 0.9143 - val_loss: 0.2116 - val_accuracy: 0.9565\n",
      "Epoch 46/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2940 - accuracy: 0.9254 - val_loss: 0.1778 - val_accuracy: 0.9664\n",
      "Epoch 47/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3027 - accuracy: 0.9217 - val_loss: 0.2036 - val_accuracy: 0.9540\n",
      "Epoch 48/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2804 - accuracy: 0.9298 - val_loss: 0.1904 - val_accuracy: 0.9565\n",
      "Epoch 49/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2832 - accuracy: 0.9243 - val_loss: 0.1899 - val_accuracy: 0.9577\n",
      "Epoch 50/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2899 - accuracy: 0.9243 - val_loss: 0.1938 - val_accuracy: 0.9565\n",
      "Epoch 51/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2639 - accuracy: 0.9347 - val_loss: 0.1792 - val_accuracy: 0.9590\n",
      "Epoch 52/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2615 - accuracy: 0.9340 - val_loss: 0.1765 - val_accuracy: 0.9577\n",
      "Epoch 53/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2550 - accuracy: 0.9337 - val_loss: 0.1837 - val_accuracy: 0.9565\n",
      "Epoch 54/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2491 - accuracy: 0.9339 - val_loss: 0.1695 - val_accuracy: 0.9590\n",
      "Epoch 55/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2545 - accuracy: 0.9343 - val_loss: 0.1620 - val_accuracy: 0.9677\n",
      "Epoch 56/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2412 - accuracy: 0.9370 - val_loss: 0.1649 - val_accuracy: 0.9664\n",
      "Epoch 57/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2363 - accuracy: 0.9383 - val_loss: 0.1584 - val_accuracy: 0.9652\n",
      "Epoch 58/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2252 - accuracy: 0.9413 - val_loss: 0.1665 - val_accuracy: 0.9652\n",
      "Epoch 59/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2276 - accuracy: 0.9390 - val_loss: 0.1626 - val_accuracy: 0.9590\n",
      "Epoch 60/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2225 - accuracy: 0.9411 - val_loss: 0.1438 - val_accuracy: 0.9677\n",
      "Epoch 61/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2110 - accuracy: 0.9471 - val_loss: 0.1561 - val_accuracy: 0.9627\n",
      "Epoch 62/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2155 - accuracy: 0.9431 - val_loss: 0.1577 - val_accuracy: 0.9689\n",
      "Epoch 63/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1918 - accuracy: 0.9542 - val_loss: 0.1628 - val_accuracy: 0.9565\n",
      "Epoch 64/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2086 - accuracy: 0.9433 - val_loss: 0.1558 - val_accuracy: 0.9590\n",
      "Epoch 65/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1997 - accuracy: 0.9485 - val_loss: 0.1472 - val_accuracy: 0.9677\n",
      "Epoch 66/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1878 - accuracy: 0.9507 - val_loss: 0.1446 - val_accuracy: 0.9664\n",
      "Epoch 67/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1892 - accuracy: 0.9503 - val_loss: 0.1340 - val_accuracy: 0.9714\n",
      "Epoch 68/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1957 - accuracy: 0.9485 - val_loss: 0.1357 - val_accuracy: 0.9689\n",
      "Epoch 69/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1872 - accuracy: 0.9507 - val_loss: 0.1550 - val_accuracy: 0.9627\n",
      "Epoch 70/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1792 - accuracy: 0.9546 - val_loss: 0.1349 - val_accuracy: 0.9664\n",
      "Epoch 71/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1831 - accuracy: 0.9528 - val_loss: 0.1445 - val_accuracy: 0.9614\n",
      "Epoch 72/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1714 - accuracy: 0.9578 - val_loss: 0.1312 - val_accuracy: 0.9701\n",
      "Epoch 73/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1592 - accuracy: 0.9574 - val_loss: 0.1396 - val_accuracy: 0.9627\n",
      "Epoch 74/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1722 - accuracy: 0.9563 - val_loss: 0.1371 - val_accuracy: 0.9701\n",
      "Epoch 75/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1670 - accuracy: 0.9556 - val_loss: 0.1405 - val_accuracy: 0.9689\n",
      "Epoch 76/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1639 - accuracy: 0.9574 - val_loss: 0.1229 - val_accuracy: 0.9714\n",
      "Epoch 77/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1667 - accuracy: 0.9556 - val_loss: 0.1265 - val_accuracy: 0.9714\n",
      "Epoch 78/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1604 - accuracy: 0.9588 - val_loss: 0.1356 - val_accuracy: 0.9701\n",
      " 5/26 [====>.........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703915865.095818  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 13ms/step\n",
      "Validation Accuracy Fold 1: 0.9626865671641791\n",
      "40/40 [==============================] - 1s 13ms/step\n",
      "Test - Accuracy: 0.6253918495297806\n",
      "---------------fold 2 -----------\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703915874.209257  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/226 [============================>.] - ETA: 0s - loss: 3.6988 - accuracy: 0.1069"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703915883.334873  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 11s 41ms/step - loss: 3.6978 - accuracy: 0.1072 - val_loss: 3.1082 - val_accuracy: 0.2578\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 2.9075 - accuracy: 0.2667 - val_loss: 2.4710 - val_accuracy: 0.4608\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 2.4908 - accuracy: 0.3643 - val_loss: 2.0136 - val_accuracy: 0.5890\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 2.1683 - accuracy: 0.4366 - val_loss: 1.7845 - val_accuracy: 0.6389\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.8976 - accuracy: 0.5086 - val_loss: 1.4795 - val_accuracy: 0.7073\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.7127 - accuracy: 0.5551 - val_loss: 1.2912 - val_accuracy: 0.7572\n",
      "Epoch 7/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.5180 - accuracy: 0.6089 - val_loss: 1.1151 - val_accuracy: 0.7709\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.3987 - accuracy: 0.6320 - val_loss: 0.9771 - val_accuracy: 0.8045\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.2707 - accuracy: 0.6774 - val_loss: 0.8850 - val_accuracy: 0.8294\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.1831 - accuracy: 0.6959 - val_loss: 0.8120 - val_accuracy: 0.8531\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.1011 - accuracy: 0.7095 - val_loss: 0.7264 - val_accuracy: 0.8742\n",
      "Epoch 12/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.0400 - accuracy: 0.7276 - val_loss: 0.6933 - val_accuracy: 0.8780\n",
      "Epoch 13/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.9586 - accuracy: 0.7435 - val_loss: 0.6286 - val_accuracy: 0.8817\n",
      "Epoch 14/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.9076 - accuracy: 0.7649 - val_loss: 0.5511 - val_accuracy: 0.9128\n",
      "Epoch 15/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.8538 - accuracy: 0.7792 - val_loss: 0.5031 - val_accuracy: 0.9153\n",
      "Epoch 16/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.8031 - accuracy: 0.7896 - val_loss: 0.4837 - val_accuracy: 0.9066\n",
      "Epoch 17/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.7572 - accuracy: 0.8045 - val_loss: 0.4724 - val_accuracy: 0.9141\n",
      "Epoch 18/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.7408 - accuracy: 0.8073 - val_loss: 0.4336 - val_accuracy: 0.9278\n",
      "Epoch 19/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.6737 - accuracy: 0.8297 - val_loss: 0.4193 - val_accuracy: 0.9278\n",
      "Epoch 20/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.6477 - accuracy: 0.8302 - val_loss: 0.3982 - val_accuracy: 0.9265\n",
      "Epoch 21/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.6253 - accuracy: 0.8341 - val_loss: 0.3906 - val_accuracy: 0.9240\n",
      "Epoch 22/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.5934 - accuracy: 0.8466 - val_loss: 0.3452 - val_accuracy: 0.9402\n",
      "Epoch 23/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.5555 - accuracy: 0.8521 - val_loss: 0.3304 - val_accuracy: 0.9415\n",
      "Epoch 24/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.5456 - accuracy: 0.8605 - val_loss: 0.3210 - val_accuracy: 0.9440\n",
      "Epoch 25/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.5315 - accuracy: 0.8579 - val_loss: 0.3099 - val_accuracy: 0.9340\n",
      "Epoch 26/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.5044 - accuracy: 0.8704 - val_loss: 0.2884 - val_accuracy: 0.9514\n",
      "Epoch 27/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.4703 - accuracy: 0.8773 - val_loss: 0.2738 - val_accuracy: 0.9465\n",
      "Epoch 28/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.4622 - accuracy: 0.8828 - val_loss: 0.2747 - val_accuracy: 0.9539\n",
      "Epoch 29/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.4421 - accuracy: 0.8845 - val_loss: 0.2677 - val_accuracy: 0.9489\n",
      "Epoch 30/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.4377 - accuracy: 0.8841 - val_loss: 0.2669 - val_accuracy: 0.9452\n",
      "Epoch 31/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.4022 - accuracy: 0.8955 - val_loss: 0.2482 - val_accuracy: 0.9601\n",
      "Epoch 32/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3997 - accuracy: 0.8967 - val_loss: 0.2378 - val_accuracy: 0.9502\n",
      "Epoch 33/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3822 - accuracy: 0.9012 - val_loss: 0.2287 - val_accuracy: 0.9601\n",
      "Epoch 34/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3677 - accuracy: 0.9081 - val_loss: 0.2166 - val_accuracy: 0.9589\n",
      "Epoch 35/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3427 - accuracy: 0.9131 - val_loss: 0.2114 - val_accuracy: 0.9614\n",
      "Epoch 36/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3485 - accuracy: 0.9106 - val_loss: 0.2065 - val_accuracy: 0.9651\n",
      "Epoch 37/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3337 - accuracy: 0.9171 - val_loss: 0.2014 - val_accuracy: 0.9589\n",
      "Epoch 38/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3309 - accuracy: 0.9117 - val_loss: 0.2153 - val_accuracy: 0.9552\n",
      "Epoch 39/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3158 - accuracy: 0.9189 - val_loss: 0.1914 - val_accuracy: 0.9714\n",
      "Epoch 40/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3036 - accuracy: 0.9206 - val_loss: 0.2014 - val_accuracy: 0.9639\n",
      "Epoch 41/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2995 - accuracy: 0.9210 - val_loss: 0.1779 - val_accuracy: 0.9714\n",
      "Epoch 42/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2965 - accuracy: 0.9216 - val_loss: 0.1671 - val_accuracy: 0.9714\n",
      "Epoch 43/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2800 - accuracy: 0.9252 - val_loss: 0.1840 - val_accuracy: 0.9577\n",
      "Epoch 44/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2796 - accuracy: 0.9274 - val_loss: 0.1728 - val_accuracy: 0.9776\n",
      "Epoch 45/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2794 - accuracy: 0.9260 - val_loss: 0.1657 - val_accuracy: 0.9751\n",
      "Epoch 46/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2710 - accuracy: 0.9294 - val_loss: 0.1720 - val_accuracy: 0.9689\n",
      "Epoch 47/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2709 - accuracy: 0.9276 - val_loss: 0.1659 - val_accuracy: 0.9714\n",
      "Epoch 48/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2477 - accuracy: 0.9346 - val_loss: 0.1610 - val_accuracy: 0.9714\n",
      "Epoch 49/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2370 - accuracy: 0.9406 - val_loss: 0.1506 - val_accuracy: 0.9726\n",
      "Epoch 50/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2513 - accuracy: 0.9348 - val_loss: 0.1593 - val_accuracy: 0.9738\n",
      "Epoch 51/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2338 - accuracy: 0.9406 - val_loss: 0.1596 - val_accuracy: 0.9676\n",
      "Epoch 52/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2276 - accuracy: 0.9440 - val_loss: 0.1515 - val_accuracy: 0.9639\n",
      "Epoch 53/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2257 - accuracy: 0.9405 - val_loss: 0.1475 - val_accuracy: 0.9701\n",
      "Epoch 54/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2078 - accuracy: 0.9462 - val_loss: 0.1568 - val_accuracy: 0.9689\n",
      "Epoch 55/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2086 - accuracy: 0.9459 - val_loss: 0.1453 - val_accuracy: 0.9726\n",
      "Epoch 56/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2195 - accuracy: 0.9442 - val_loss: 0.1483 - val_accuracy: 0.9714\n",
      "Epoch 57/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2030 - accuracy: 0.9492 - val_loss: 0.1575 - val_accuracy: 0.9664\n",
      "Epoch 58/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1983 - accuracy: 0.9498 - val_loss: 0.1326 - val_accuracy: 0.9726\n",
      "Epoch 59/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2028 - accuracy: 0.9492 - val_loss: 0.1331 - val_accuracy: 0.9763\n",
      "Epoch 60/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1838 - accuracy: 0.9525 - val_loss: 0.1305 - val_accuracy: 0.9763\n",
      "Epoch 61/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1770 - accuracy: 0.9578 - val_loss: 0.1415 - val_accuracy: 0.9726\n",
      "Epoch 62/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1811 - accuracy: 0.9531 - val_loss: 0.1591 - val_accuracy: 0.9701\n",
      "Epoch 63/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1668 - accuracy: 0.9582 - val_loss: 0.1340 - val_accuracy: 0.9751\n",
      "Epoch 64/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1771 - accuracy: 0.9549 - val_loss: 0.1289 - val_accuracy: 0.9726\n",
      "Epoch 65/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1707 - accuracy: 0.9541 - val_loss: 0.1363 - val_accuracy: 0.9751\n",
      "Epoch 66/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1641 - accuracy: 0.9579 - val_loss: 0.1307 - val_accuracy: 0.9726\n",
      "Epoch 67/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1694 - accuracy: 0.9579 - val_loss: 0.1336 - val_accuracy: 0.9738\n",
      "Epoch 68/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1668 - accuracy: 0.9568 - val_loss: 0.1232 - val_accuracy: 0.9788\n",
      "Epoch 69/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1634 - accuracy: 0.9584 - val_loss: 0.1227 - val_accuracy: 0.9763\n",
      "Epoch 70/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1596 - accuracy: 0.9590 - val_loss: 0.1249 - val_accuracy: 0.9738\n",
      "Epoch 71/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1521 - accuracy: 0.9639 - val_loss: 0.1340 - val_accuracy: 0.9751\n",
      "Epoch 72/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1464 - accuracy: 0.9628 - val_loss: 0.1332 - val_accuracy: 0.9763\n",
      "Epoch 73/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1450 - accuracy: 0.9629 - val_loss: 0.1300 - val_accuracy: 0.9714\n",
      "Epoch 74/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1541 - accuracy: 0.9611 - val_loss: 0.1261 - val_accuracy: 0.9751\n",
      "Epoch 75/200\n",
      "226/226 [==============================] - 9s 40ms/step - loss: 0.1432 - accuracy: 0.9632 - val_loss: 0.1278 - val_accuracy: 0.9714\n",
      "Epoch 76/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1432 - accuracy: 0.9638 - val_loss: 0.1256 - val_accuracy: 0.9701\n",
      "Epoch 77/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1397 - accuracy: 0.9643 - val_loss: 0.1250 - val_accuracy: 0.9751\n",
      "Epoch 78/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1297 - accuracy: 0.9691 - val_loss: 0.1193 - val_accuracy: 0.9763\n",
      "Epoch 79/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1427 - accuracy: 0.9624 - val_loss: 0.1275 - val_accuracy: 0.9763\n",
      "Epoch 80/200\n",
      "226/226 [==============================] - 9s 40ms/step - loss: 0.1287 - accuracy: 0.9676 - val_loss: 0.1213 - val_accuracy: 0.9776\n",
      "Epoch 81/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1215 - accuracy: 0.9683 - val_loss: 0.1269 - val_accuracy: 0.9738\n",
      "Epoch 82/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1312 - accuracy: 0.9667 - val_loss: 0.1159 - val_accuracy: 0.9776\n",
      "Epoch 83/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1285 - accuracy: 0.9682 - val_loss: 0.1280 - val_accuracy: 0.9776\n",
      "Epoch 84/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1234 - accuracy: 0.9680 - val_loss: 0.1204 - val_accuracy: 0.9726\n",
      "Epoch 85/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1209 - accuracy: 0.9694 - val_loss: 0.1302 - val_accuracy: 0.9751\n",
      "Epoch 86/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1210 - accuracy: 0.9698 - val_loss: 0.1168 - val_accuracy: 0.9751\n",
      "Epoch 87/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1235 - accuracy: 0.9689 - val_loss: 0.1313 - val_accuracy: 0.9714\n",
      "Epoch 88/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1331 - accuracy: 0.9665 - val_loss: 0.1321 - val_accuracy: 0.9689\n",
      "Epoch 89/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1204 - accuracy: 0.9711 - val_loss: 0.1185 - val_accuracy: 0.9763\n",
      "Epoch 90/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1128 - accuracy: 0.9718 - val_loss: 0.1174 - val_accuracy: 0.9788\n",
      "Epoch 91/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1198 - accuracy: 0.9678 - val_loss: 0.1185 - val_accuracy: 0.9738\n",
      "Epoch 92/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1145 - accuracy: 0.9703 - val_loss: 0.1296 - val_accuracy: 0.9714\n",
      "Epoch 93/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1078 - accuracy: 0.9733 - val_loss: 0.1093 - val_accuracy: 0.9801\n",
      "Epoch 94/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1076 - accuracy: 0.9712 - val_loss: 0.1165 - val_accuracy: 0.9788\n",
      "Epoch 95/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1066 - accuracy: 0.9721 - val_loss: 0.1170 - val_accuracy: 0.9738\n",
      "Epoch 96/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1055 - accuracy: 0.9727 - val_loss: 0.1227 - val_accuracy: 0.9751\n",
      "Epoch 97/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1060 - accuracy: 0.9736 - val_loss: 0.1098 - val_accuracy: 0.9826\n",
      "Epoch 98/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1062 - accuracy: 0.9736 - val_loss: 0.1185 - val_accuracy: 0.9776\n",
      "Epoch 99/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1103 - accuracy: 0.9722 - val_loss: 0.1254 - val_accuracy: 0.9738\n",
      "Epoch 100/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1054 - accuracy: 0.9716 - val_loss: 0.1192 - val_accuracy: 0.9751\n",
      "Epoch 101/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1003 - accuracy: 0.9741 - val_loss: 0.1102 - val_accuracy: 0.9801\n",
      "Epoch 102/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1028 - accuracy: 0.9751 - val_loss: 0.1072 - val_accuracy: 0.9763\n",
      "Epoch 103/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.1016 - accuracy: 0.9733 - val_loss: 0.1137 - val_accuracy: 0.9751\n",
      "Epoch 104/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0945 - accuracy: 0.9745 - val_loss: 0.1196 - val_accuracy: 0.9738\n",
      "Epoch 105/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0948 - accuracy: 0.9736 - val_loss: 0.1174 - val_accuracy: 0.9726\n",
      "Epoch 106/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0954 - accuracy: 0.9780 - val_loss: 0.1269 - val_accuracy: 0.9701\n",
      "Epoch 107/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0938 - accuracy: 0.9763 - val_loss: 0.1120 - val_accuracy: 0.9763\n",
      "Epoch 108/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0925 - accuracy: 0.9761 - val_loss: 0.1195 - val_accuracy: 0.9738\n",
      "Epoch 109/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0820 - accuracy: 0.9798 - val_loss: 0.1010 - val_accuracy: 0.9801\n",
      "Epoch 110/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0915 - accuracy: 0.9769 - val_loss: 0.1240 - val_accuracy: 0.9738\n",
      "Epoch 111/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0914 - accuracy: 0.9751 - val_loss: 0.1237 - val_accuracy: 0.9751\n",
      "Epoch 112/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0876 - accuracy: 0.9768 - val_loss: 0.1095 - val_accuracy: 0.9826\n",
      "Epoch 113/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0819 - accuracy: 0.9792 - val_loss: 0.1195 - val_accuracy: 0.9751\n",
      "Epoch 114/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0906 - accuracy: 0.9757 - val_loss: 0.0948 - val_accuracy: 0.9801\n",
      "Epoch 115/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0892 - accuracy: 0.9776 - val_loss: 0.1054 - val_accuracy: 0.9788\n",
      "Epoch 116/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0744 - accuracy: 0.9816 - val_loss: 0.1197 - val_accuracy: 0.9763\n",
      "Epoch 117/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0733 - accuracy: 0.9813 - val_loss: 0.0979 - val_accuracy: 0.9826\n",
      "Epoch 118/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0808 - accuracy: 0.9790 - val_loss: 0.1019 - val_accuracy: 0.9751\n",
      "Epoch 119/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0819 - accuracy: 0.9795 - val_loss: 0.1164 - val_accuracy: 0.9751\n",
      "Epoch 120/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0848 - accuracy: 0.9763 - val_loss: 0.1127 - val_accuracy: 0.9788\n",
      "Epoch 121/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0942 - accuracy: 0.9761 - val_loss: 0.1187 - val_accuracy: 0.9801\n",
      "Epoch 122/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.0919 - accuracy: 0.9774 - val_loss: 0.1180 - val_accuracy: 0.9751\n",
      " 5/26 [====>.........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703916937.446970  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 13ms/step\n",
      "Validation Accuracy Fold 2: 0.9825653798256538\n",
      "40/40 [==============================] - 1s 13ms/step\n",
      "Test - Accuracy: 0.5862068965517241\n",
      "---------------fold 3 -----------\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703916945.241772  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/226 [============================>.] - ETA: 0s - loss: 3.7129 - accuracy: 0.1032"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703916954.428472  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 12s 41ms/step - loss: 3.7113 - accuracy: 0.1035 - val_loss: 3.1603 - val_accuracy: 0.2416\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 3.0205 - accuracy: 0.2452 - val_loss: 2.6035 - val_accuracy: 0.4247\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 2.5696 - accuracy: 0.3568 - val_loss: 2.1639 - val_accuracy: 0.5255\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 2.2505 - accuracy: 0.4265 - val_loss: 1.9257 - val_accuracy: 0.5778\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 2.0240 - accuracy: 0.4769 - val_loss: 1.6590 - val_accuracy: 0.6202\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.8029 - accuracy: 0.5266 - val_loss: 1.4513 - val_accuracy: 0.6899\n",
      "Epoch 7/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.6482 - accuracy: 0.5683 - val_loss: 1.3279 - val_accuracy: 0.7061\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.4902 - accuracy: 0.6085 - val_loss: 1.1568 - val_accuracy: 0.7497\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.3857 - accuracy: 0.6375 - val_loss: 1.0551 - val_accuracy: 0.7833\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.2883 - accuracy: 0.6562 - val_loss: 0.9668 - val_accuracy: 0.8032\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.2155 - accuracy: 0.6792 - val_loss: 0.8692 - val_accuracy: 0.8207\n",
      "Epoch 12/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.0993 - accuracy: 0.7090 - val_loss: 0.7982 - val_accuracy: 0.8294\n",
      "Epoch 13/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.0521 - accuracy: 0.7214 - val_loss: 0.7268 - val_accuracy: 0.8630\n",
      "Epoch 14/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.9767 - accuracy: 0.7381 - val_loss: 0.6804 - val_accuracy: 0.8593\n",
      "Epoch 15/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.9294 - accuracy: 0.7524 - val_loss: 0.6228 - val_accuracy: 0.8817\n",
      "Epoch 16/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.8833 - accuracy: 0.7688 - val_loss: 0.5706 - val_accuracy: 0.8854\n",
      "Epoch 17/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.8315 - accuracy: 0.7774 - val_loss: 0.5583 - val_accuracy: 0.8879\n",
      "Epoch 18/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.7738 - accuracy: 0.7970 - val_loss: 0.5052 - val_accuracy: 0.8991\n",
      "Epoch 19/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.7490 - accuracy: 0.7961 - val_loss: 0.4733 - val_accuracy: 0.9166\n",
      "Epoch 20/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.7174 - accuracy: 0.8096 - val_loss: 0.4517 - val_accuracy: 0.9166\n",
      "Epoch 21/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6794 - accuracy: 0.8221 - val_loss: 0.4397 - val_accuracy: 0.9128\n",
      "Epoch 22/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6240 - accuracy: 0.8394 - val_loss: 0.3995 - val_accuracy: 0.9240\n",
      "Epoch 23/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6189 - accuracy: 0.8326 - val_loss: 0.3936 - val_accuracy: 0.9303\n",
      "Epoch 24/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5771 - accuracy: 0.8488 - val_loss: 0.3499 - val_accuracy: 0.9365\n",
      "Epoch 25/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5660 - accuracy: 0.8492 - val_loss: 0.3318 - val_accuracy: 0.9328\n",
      "Epoch 26/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.5496 - accuracy: 0.8593 - val_loss: 0.3254 - val_accuracy: 0.9415\n",
      "Epoch 27/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.5166 - accuracy: 0.8640 - val_loss: 0.2998 - val_accuracy: 0.9427\n",
      "Epoch 28/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5064 - accuracy: 0.8644 - val_loss: 0.3104 - val_accuracy: 0.9377\n",
      "Epoch 29/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4689 - accuracy: 0.8724 - val_loss: 0.3089 - val_accuracy: 0.9415\n",
      "Epoch 30/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4610 - accuracy: 0.8802 - val_loss: 0.2933 - val_accuracy: 0.9390\n",
      "Epoch 31/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4483 - accuracy: 0.8828 - val_loss: 0.2705 - val_accuracy: 0.9415\n",
      "Epoch 32/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4362 - accuracy: 0.8832 - val_loss: 0.2472 - val_accuracy: 0.9514\n",
      "Epoch 33/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4188 - accuracy: 0.8884 - val_loss: 0.2613 - val_accuracy: 0.9465\n",
      "Epoch 34/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4191 - accuracy: 0.8893 - val_loss: 0.2381 - val_accuracy: 0.9452\n",
      "Epoch 35/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3930 - accuracy: 0.8978 - val_loss: 0.2155 - val_accuracy: 0.9539\n",
      "Epoch 36/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3727 - accuracy: 0.9037 - val_loss: 0.2119 - val_accuracy: 0.9564\n",
      "Epoch 37/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3742 - accuracy: 0.9043 - val_loss: 0.2524 - val_accuracy: 0.9477\n",
      "Epoch 38/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3802 - accuracy: 0.8979 - val_loss: 0.2313 - val_accuracy: 0.9440\n",
      "Epoch 39/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3491 - accuracy: 0.9047 - val_loss: 0.2238 - val_accuracy: 0.9465\n",
      "Epoch 40/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3388 - accuracy: 0.9108 - val_loss: 0.1986 - val_accuracy: 0.9601\n",
      "Epoch 41/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3463 - accuracy: 0.9066 - val_loss: 0.1898 - val_accuracy: 0.9614\n",
      "Epoch 42/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3254 - accuracy: 0.9131 - val_loss: 0.2486 - val_accuracy: 0.9352\n",
      "Epoch 43/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3162 - accuracy: 0.9146 - val_loss: 0.1947 - val_accuracy: 0.9601\n",
      "Epoch 44/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3135 - accuracy: 0.9191 - val_loss: 0.1819 - val_accuracy: 0.9601\n",
      "Epoch 45/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3064 - accuracy: 0.9249 - val_loss: 0.1946 - val_accuracy: 0.9589\n",
      "Epoch 46/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2906 - accuracy: 0.9245 - val_loss: 0.1798 - val_accuracy: 0.9564\n",
      "Epoch 47/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2783 - accuracy: 0.9271 - val_loss: 0.1677 - val_accuracy: 0.9589\n",
      "Epoch 48/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2774 - accuracy: 0.9307 - val_loss: 0.1814 - val_accuracy: 0.9626\n",
      "Epoch 49/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2650 - accuracy: 0.9312 - val_loss: 0.1651 - val_accuracy: 0.9689\n",
      "Epoch 50/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2621 - accuracy: 0.9332 - val_loss: 0.1672 - val_accuracy: 0.9676\n",
      "Epoch 51/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2510 - accuracy: 0.9312 - val_loss: 0.1851 - val_accuracy: 0.9539\n",
      "Epoch 52/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2524 - accuracy: 0.9337 - val_loss: 0.1645 - val_accuracy: 0.9626\n",
      "Epoch 53/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2469 - accuracy: 0.9344 - val_loss: 0.1659 - val_accuracy: 0.9577\n",
      "Epoch 54/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2334 - accuracy: 0.9401 - val_loss: 0.1577 - val_accuracy: 0.9614\n",
      "Epoch 55/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2177 - accuracy: 0.9448 - val_loss: 0.1554 - val_accuracy: 0.9626\n",
      "Epoch 56/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2298 - accuracy: 0.9398 - val_loss: 0.1641 - val_accuracy: 0.9564\n",
      "Epoch 57/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2185 - accuracy: 0.9452 - val_loss: 0.1433 - val_accuracy: 0.9701\n",
      "Epoch 58/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2277 - accuracy: 0.9394 - val_loss: 0.1384 - val_accuracy: 0.9651\n",
      "Epoch 59/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2009 - accuracy: 0.9477 - val_loss: 0.1313 - val_accuracy: 0.9714\n",
      "Epoch 60/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2157 - accuracy: 0.9438 - val_loss: 0.1380 - val_accuracy: 0.9639\n",
      "Epoch 61/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2201 - accuracy: 0.9394 - val_loss: 0.1460 - val_accuracy: 0.9651\n",
      "Epoch 62/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2028 - accuracy: 0.9449 - val_loss: 0.1360 - val_accuracy: 0.9676\n",
      "Epoch 63/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2001 - accuracy: 0.9460 - val_loss: 0.1393 - val_accuracy: 0.9689\n",
      "Epoch 64/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1936 - accuracy: 0.9505 - val_loss: 0.1250 - val_accuracy: 0.9714\n",
      "Epoch 65/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1845 - accuracy: 0.9528 - val_loss: 0.1400 - val_accuracy: 0.9676\n",
      "Epoch 66/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1743 - accuracy: 0.9572 - val_loss: 0.1346 - val_accuracy: 0.9689\n",
      "Epoch 67/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1810 - accuracy: 0.9524 - val_loss: 0.1236 - val_accuracy: 0.9714\n",
      "Epoch 68/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1831 - accuracy: 0.9534 - val_loss: 0.1393 - val_accuracy: 0.9664\n",
      "Epoch 69/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1804 - accuracy: 0.9513 - val_loss: 0.1135 - val_accuracy: 0.9763\n",
      "Epoch 70/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1645 - accuracy: 0.9585 - val_loss: 0.1189 - val_accuracy: 0.9751\n",
      "Epoch 71/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1645 - accuracy: 0.9599 - val_loss: 0.1256 - val_accuracy: 0.9664\n",
      "Epoch 72/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1691 - accuracy: 0.9545 - val_loss: 0.1281 - val_accuracy: 0.9664\n",
      "Epoch 73/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1605 - accuracy: 0.9563 - val_loss: 0.1261 - val_accuracy: 0.9714\n",
      "Epoch 74/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1533 - accuracy: 0.9593 - val_loss: 0.1230 - val_accuracy: 0.9689\n",
      "Epoch 75/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1717 - accuracy: 0.9555 - val_loss: 0.1170 - val_accuracy: 0.9664\n",
      "Epoch 76/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1461 - accuracy: 0.9628 - val_loss: 0.1086 - val_accuracy: 0.9751\n",
      "Epoch 77/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1661 - accuracy: 0.9555 - val_loss: 0.1135 - val_accuracy: 0.9714\n",
      "Epoch 78/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1474 - accuracy: 0.9610 - val_loss: 0.1137 - val_accuracy: 0.9738\n",
      "Epoch 79/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1544 - accuracy: 0.9610 - val_loss: 0.1100 - val_accuracy: 0.9751\n",
      "Epoch 80/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1439 - accuracy: 0.9633 - val_loss: 0.1106 - val_accuracy: 0.9701\n",
      "Epoch 81/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1412 - accuracy: 0.9636 - val_loss: 0.1013 - val_accuracy: 0.9763\n",
      "Epoch 82/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1333 - accuracy: 0.9672 - val_loss: 0.1118 - val_accuracy: 0.9738\n",
      "Epoch 83/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1431 - accuracy: 0.9638 - val_loss: 0.1085 - val_accuracy: 0.9763\n",
      "Epoch 84/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1309 - accuracy: 0.9660 - val_loss: 0.1044 - val_accuracy: 0.9726\n",
      "Epoch 85/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1306 - accuracy: 0.9675 - val_loss: 0.1141 - val_accuracy: 0.9714\n",
      "Epoch 86/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1422 - accuracy: 0.9635 - val_loss: 0.1452 - val_accuracy: 0.9651\n",
      "Epoch 87/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1459 - accuracy: 0.9606 - val_loss: 0.0935 - val_accuracy: 0.9751\n",
      "Epoch 88/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1259 - accuracy: 0.9687 - val_loss: 0.1057 - val_accuracy: 0.9701\n",
      "Epoch 89/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1243 - accuracy: 0.9682 - val_loss: 0.1146 - val_accuracy: 0.9738\n",
      "Epoch 90/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1230 - accuracy: 0.9690 - val_loss: 0.1018 - val_accuracy: 0.9701\n",
      "Epoch 91/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1269 - accuracy: 0.9653 - val_loss: 0.1054 - val_accuracy: 0.9714\n",
      "Epoch 92/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1110 - accuracy: 0.9732 - val_loss: 0.1117 - val_accuracy: 0.9738\n",
      "Epoch 93/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1316 - accuracy: 0.9672 - val_loss: 0.1059 - val_accuracy: 0.9726\n",
      "Epoch 94/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1157 - accuracy: 0.9685 - val_loss: 0.1175 - val_accuracy: 0.9676\n",
      "Epoch 95/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1268 - accuracy: 0.9669 - val_loss: 0.1074 - val_accuracy: 0.9714\n",
      "Epoch 96/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1164 - accuracy: 0.9701 - val_loss: 0.1226 - val_accuracy: 0.9614\n",
      "Epoch 97/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1261 - accuracy: 0.9668 - val_loss: 0.1023 - val_accuracy: 0.9738\n",
      " 5/26 [====>.........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703917795.934605  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 13ms/step\n",
      "Validation Accuracy Fold 3: 0.9738480697384807\n",
      "40/40 [==============================] - 1s 13ms/step\n",
      "Test - Accuracy: 0.6199059561128527\n",
      "---------------fold 4 -----------\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703917802.920130  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/226 [============================>.] - ETA: 0s - loss: 3.7242 - accuracy: 0.1064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703917812.137490  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 12s 41ms/step - loss: 3.7229 - accuracy: 0.1067 - val_loss: 3.1992 - val_accuracy: 0.2578\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 2.9981 - accuracy: 0.2576 - val_loss: 2.6209 - val_accuracy: 0.4035\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 2.5416 - accuracy: 0.3657 - val_loss: 2.1777 - val_accuracy: 0.4894\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 2.2139 - accuracy: 0.4386 - val_loss: 1.9003 - val_accuracy: 0.5529\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.9737 - accuracy: 0.4914 - val_loss: 1.5933 - val_accuracy: 0.6650\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.7836 - accuracy: 0.5360 - val_loss: 1.4145 - val_accuracy: 0.7049\n",
      "Epoch 7/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.6059 - accuracy: 0.5794 - val_loss: 1.2671 - val_accuracy: 0.7484\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.4909 - accuracy: 0.6039 - val_loss: 1.1172 - val_accuracy: 0.7758\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.3620 - accuracy: 0.6399 - val_loss: 1.0171 - val_accuracy: 0.8120\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.2644 - accuracy: 0.6652 - val_loss: 0.8933 - val_accuracy: 0.8356\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.1463 - accuracy: 0.7025 - val_loss: 0.8025 - val_accuracy: 0.8618\n",
      "Epoch 12/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 1.1013 - accuracy: 0.7088 - val_loss: 0.7028 - val_accuracy: 0.8842\n",
      "Epoch 13/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 1.0099 - accuracy: 0.7338 - val_loss: 0.7095 - val_accuracy: 0.8580\n",
      "Epoch 14/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.9626 - accuracy: 0.7420 - val_loss: 0.6408 - val_accuracy: 0.8867\n",
      "Epoch 15/200\n",
      "226/226 [==============================] - 8s 38ms/step - loss: 0.9007 - accuracy: 0.7649 - val_loss: 0.5821 - val_accuracy: 0.8904\n",
      "Epoch 16/200\n",
      "226/226 [==============================] - 8s 37ms/step - loss: 0.8528 - accuracy: 0.7737 - val_loss: 0.5350 - val_accuracy: 0.9091\n",
      "Epoch 17/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.7930 - accuracy: 0.7824 - val_loss: 0.4847 - val_accuracy: 0.9054\n",
      "Epoch 18/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.7775 - accuracy: 0.7951 - val_loss: 0.4745 - val_accuracy: 0.9153\n",
      "Epoch 19/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.7230 - accuracy: 0.8074 - val_loss: 0.4698 - val_accuracy: 0.9203\n",
      "Epoch 20/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.7042 - accuracy: 0.8100 - val_loss: 0.4204 - val_accuracy: 0.9166\n",
      "Epoch 21/200\n",
      "226/226 [==============================] - 8s 38ms/step - loss: 0.6627 - accuracy: 0.8237 - val_loss: 0.4008 - val_accuracy: 0.9290\n",
      "Epoch 22/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6533 - accuracy: 0.8222 - val_loss: 0.3868 - val_accuracy: 0.9290\n",
      "Epoch 23/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.6224 - accuracy: 0.8284 - val_loss: 0.3608 - val_accuracy: 0.9340\n",
      "Epoch 24/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5857 - accuracy: 0.8410 - val_loss: 0.3254 - val_accuracy: 0.9328\n",
      "Epoch 25/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5570 - accuracy: 0.8550 - val_loss: 0.3204 - val_accuracy: 0.9377\n",
      "Epoch 26/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5526 - accuracy: 0.8547 - val_loss: 0.3108 - val_accuracy: 0.9440\n",
      "Epoch 27/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.5163 - accuracy: 0.8633 - val_loss: 0.2984 - val_accuracy: 0.9527\n",
      "Epoch 28/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4944 - accuracy: 0.8679 - val_loss: 0.2946 - val_accuracy: 0.9377\n",
      "Epoch 29/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4785 - accuracy: 0.8720 - val_loss: 0.2518 - val_accuracy: 0.9577\n",
      "Epoch 30/200\n",
      "226/226 [==============================] - 8s 38ms/step - loss: 0.4519 - accuracy: 0.8788 - val_loss: 0.2576 - val_accuracy: 0.9502\n",
      "Epoch 31/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4500 - accuracy: 0.8850 - val_loss: 0.2384 - val_accuracy: 0.9577\n",
      "Epoch 32/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4424 - accuracy: 0.8807 - val_loss: 0.2293 - val_accuracy: 0.9527\n",
      "Epoch 33/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.4189 - accuracy: 0.8933 - val_loss: 0.2299 - val_accuracy: 0.9577\n",
      "Epoch 34/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3962 - accuracy: 0.8949 - val_loss: 0.2153 - val_accuracy: 0.9577\n",
      "Epoch 35/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3846 - accuracy: 0.8964 - val_loss: 0.2162 - val_accuracy: 0.9589\n",
      "Epoch 36/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3725 - accuracy: 0.9001 - val_loss: 0.2066 - val_accuracy: 0.9601\n",
      "Epoch 37/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3693 - accuracy: 0.8987 - val_loss: 0.2076 - val_accuracy: 0.9527\n",
      "Epoch 38/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3500 - accuracy: 0.9061 - val_loss: 0.1976 - val_accuracy: 0.9626\n",
      "Epoch 39/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3445 - accuracy: 0.9138 - val_loss: 0.1986 - val_accuracy: 0.9614\n",
      "Epoch 40/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3481 - accuracy: 0.9099 - val_loss: 0.1893 - val_accuracy: 0.9614\n",
      "Epoch 41/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.3264 - accuracy: 0.9124 - val_loss: 0.1758 - val_accuracy: 0.9689\n",
      "Epoch 42/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3226 - accuracy: 0.9133 - val_loss: 0.2036 - val_accuracy: 0.9514\n",
      "Epoch 43/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2974 - accuracy: 0.9217 - val_loss: 0.1907 - val_accuracy: 0.9589\n",
      "Epoch 44/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.3013 - accuracy: 0.9166 - val_loss: 0.1723 - val_accuracy: 0.9651\n",
      "Epoch 45/200\n",
      "226/226 [==============================] - 9s 38ms/step - loss: 0.2896 - accuracy: 0.9220 - val_loss: 0.1729 - val_accuracy: 0.9552\n",
      "Epoch 46/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2856 - accuracy: 0.9249 - val_loss: 0.1741 - val_accuracy: 0.9626\n",
      "Epoch 47/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2867 - accuracy: 0.9283 - val_loss: 0.1799 - val_accuracy: 0.9601\n",
      "Epoch 48/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2701 - accuracy: 0.9272 - val_loss: 0.1498 - val_accuracy: 0.9676\n",
      "Epoch 49/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2677 - accuracy: 0.9260 - val_loss: 0.1409 - val_accuracy: 0.9738\n",
      "Epoch 50/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2673 - accuracy: 0.9287 - val_loss: 0.1391 - val_accuracy: 0.9714\n",
      "Epoch 51/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2459 - accuracy: 0.9393 - val_loss: 0.1490 - val_accuracy: 0.9614\n",
      "Epoch 52/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2503 - accuracy: 0.9304 - val_loss: 0.1521 - val_accuracy: 0.9689\n",
      "Epoch 53/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2314 - accuracy: 0.9394 - val_loss: 0.1352 - val_accuracy: 0.9763\n",
      "Epoch 54/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2392 - accuracy: 0.9400 - val_loss: 0.1363 - val_accuracy: 0.9664\n",
      "Epoch 55/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2193 - accuracy: 0.9413 - val_loss: 0.1330 - val_accuracy: 0.9751\n",
      "Epoch 56/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2168 - accuracy: 0.9445 - val_loss: 0.1306 - val_accuracy: 0.9664\n",
      "Epoch 57/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2136 - accuracy: 0.9481 - val_loss: 0.1299 - val_accuracy: 0.9676\n",
      "Epoch 58/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2201 - accuracy: 0.9405 - val_loss: 0.1298 - val_accuracy: 0.9701\n",
      "Epoch 59/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2195 - accuracy: 0.9418 - val_loss: 0.1217 - val_accuracy: 0.9763\n",
      "Epoch 60/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2145 - accuracy: 0.9433 - val_loss: 0.1406 - val_accuracy: 0.9651\n",
      "Epoch 61/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2126 - accuracy: 0.9444 - val_loss: 0.1181 - val_accuracy: 0.9776\n",
      "Epoch 62/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2051 - accuracy: 0.9454 - val_loss: 0.1309 - val_accuracy: 0.9676\n",
      "Epoch 63/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.2091 - accuracy: 0.9442 - val_loss: 0.1182 - val_accuracy: 0.9801\n",
      "Epoch 64/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1965 - accuracy: 0.9467 - val_loss: 0.1284 - val_accuracy: 0.9738\n",
      "Epoch 65/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1879 - accuracy: 0.9519 - val_loss: 0.1363 - val_accuracy: 0.9639\n",
      "Epoch 66/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1846 - accuracy: 0.9546 - val_loss: 0.1130 - val_accuracy: 0.9664\n",
      "Epoch 67/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1945 - accuracy: 0.9476 - val_loss: 0.1243 - val_accuracy: 0.9751\n",
      "Epoch 68/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1749 - accuracy: 0.9534 - val_loss: 0.1265 - val_accuracy: 0.9614\n",
      "Epoch 69/200\n",
      "226/226 [==============================] - 9s 40ms/step - loss: 0.1645 - accuracy: 0.9586 - val_loss: 0.1115 - val_accuracy: 0.9763\n",
      "Epoch 70/200\n",
      "226/226 [==============================] - 9s 40ms/step - loss: 0.1665 - accuracy: 0.9566 - val_loss: 0.1105 - val_accuracy: 0.9738\n",
      "Epoch 71/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1910 - accuracy: 0.9496 - val_loss: 0.1295 - val_accuracy: 0.9689\n",
      "Epoch 72/200\n",
      "226/226 [==============================] - 9s 40ms/step - loss: 0.1689 - accuracy: 0.9572 - val_loss: 0.1026 - val_accuracy: 0.9776\n",
      "Epoch 73/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1664 - accuracy: 0.9577 - val_loss: 0.1143 - val_accuracy: 0.9751\n",
      "Epoch 74/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1543 - accuracy: 0.9603 - val_loss: 0.1021 - val_accuracy: 0.9726\n",
      "Epoch 75/200\n",
      "226/226 [==============================] - 9s 40ms/step - loss: 0.1685 - accuracy: 0.9567 - val_loss: 0.1072 - val_accuracy: 0.9689\n",
      "Epoch 76/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1642 - accuracy: 0.9590 - val_loss: 0.1139 - val_accuracy: 0.9664\n",
      "Epoch 77/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1725 - accuracy: 0.9502 - val_loss: 0.1153 - val_accuracy: 0.9689\n",
      "Epoch 78/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1646 - accuracy: 0.9572 - val_loss: 0.1164 - val_accuracy: 0.9701\n",
      "Epoch 79/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1506 - accuracy: 0.9620 - val_loss: 0.1074 - val_accuracy: 0.9751\n",
      "Epoch 80/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1545 - accuracy: 0.9600 - val_loss: 0.1056 - val_accuracy: 0.9751\n",
      "Epoch 81/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1466 - accuracy: 0.9589 - val_loss: 0.1062 - val_accuracy: 0.9726\n",
      "Epoch 82/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1499 - accuracy: 0.9586 - val_loss: 0.1141 - val_accuracy: 0.9751\n",
      "Epoch 83/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1421 - accuracy: 0.9617 - val_loss: 0.1010 - val_accuracy: 0.9763\n",
      "Epoch 84/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1326 - accuracy: 0.9675 - val_loss: 0.1079 - val_accuracy: 0.9726\n",
      "Epoch 85/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1320 - accuracy: 0.9667 - val_loss: 0.1021 - val_accuracy: 0.9751\n",
      "Epoch 86/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1270 - accuracy: 0.9649 - val_loss: 0.1034 - val_accuracy: 0.9738\n",
      "Epoch 87/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1367 - accuracy: 0.9632 - val_loss: 0.1065 - val_accuracy: 0.9763\n",
      "Epoch 88/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1348 - accuracy: 0.9644 - val_loss: 0.0992 - val_accuracy: 0.9763\n",
      "Epoch 89/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1364 - accuracy: 0.9632 - val_loss: 0.1181 - val_accuracy: 0.9714\n",
      "Epoch 90/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1305 - accuracy: 0.9656 - val_loss: 0.1307 - val_accuracy: 0.9664\n",
      "Epoch 91/200\n",
      "226/226 [==============================] - 9s 39ms/step - loss: 0.1384 - accuracy: 0.9633 - val_loss: 0.1073 - val_accuracy: 0.9701\n",
      " 9/26 [=========>....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1703918599.979774  335154 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"103\" frequency: 3187 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 13ms/step\n",
      "Validation Accuracy Fold 4: 0.9738480697384807\n",
      "40/40 [==============================] - 1s 13ms/step\n",
      "Test - Accuracy: 0.6112852664576802\n",
      "---------------fold 5 -----------\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "# training the total data\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Flatten, Dense,Input,  Dropout, BatchNormalization, GRU, Attention,Concatenate, Conv1D, Masking, Embedding\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32 # 64\n",
    "NO_CLASSES = 60 #Changes according to no of class\n",
    "MAX_FRMAE = 164\n",
    "\n",
    "learning_rate = 0.00003\n",
    "input_shape = (MAX_FRMAE,225) # pose hands\n",
    "#input_shape = (MAX_FRMAE,150) # pose hands xy only\n",
    "# input_shape = (MAX_FRMAE,1629)\n",
    "#input_shape = (MAX_FRMAE,1086) # all points but x y only\n",
    "#input_shape = (MAX_FRMAE,99)  #pose\n",
    "#input_shape = (MAX_FRMAE,66)  #pose only xy\n",
    "\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "print(y_shape)\n",
    "#y_onehot = to_categorical(y_shape).astype(int)\n",
    "label_encoder=LabelEncoder()\n",
    "y_encoded =label_encoder.fit_transform(y_shape)\n",
    "y_onehot =to_categorical(y_encoded,num_classes=NO_CLASSES)\n",
    "print(len(y_onehot))\n",
    "\n",
    "\n",
    "\n",
    "all_accuracies = []\n",
    "x_shapeML = np.array(x_shape) #.astype(int)\n",
    "x_train=x_shapeML\n",
    "y_train =y_onehot\n",
    "\n",
    "test_x_shapeML = np.array(test_x_shape) #.astype(int)\n",
    "x_val=test_x_shapeML\n",
    "\n",
    "#y_val = to_categorical(test_y_shape).astype(int)\n",
    "y_encoded =label_encoder.fit_transform(test_y_shape)\n",
    "y_val =to_categorical(y_encoded,num_classes=NO_CLASSES)\n",
    "\n",
    "\n",
    "print('Train shape: ',x_train.shape, y_train.shape,'Test Shape: ', x_val.shape, y_val.shape)\n",
    "\n",
    "\n",
    "# Define and compile the model\n",
    "\n",
    "\n",
    "num_folds = 10\n",
    "# Train the model\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "\n",
    "i = 1\n",
    "\n",
    "all_accuracies = []\n",
    "test_accuracies = []\n",
    "conf_matrixes = []\n",
    "bestTestAccuracy=0\n",
    "bestTest=0\n",
    "\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "\n",
    "    input = Input(shape=input_shape)\n",
    "    bd1 = Bidirectional(LSTM(units=64, return_sequences=True))(input)\n",
    "    # bn1 = BatchNormalization()(bd1)\n",
    "    bd11 = Dropout(0.3)(bd1)\n",
    "\n",
    "    bd2 = Bidirectional(LSTM(units=64, return_sequences=True))(input)\n",
    "    # bd22 = BatchNormalization()(bd2)\n",
    "\n",
    "    att = Attention(use_scale=True)([bd2, bd11])\n",
    "\n",
    "    merged = Concatenate(axis=-1)([bd2, att])\n",
    "\n",
    "    flat = Flatten()(merged)\n",
    "    dense = Dense(units=64, activation='silu')(flat)\n",
    "    dp2 = Dropout(0.3)(dense)\n",
    "    out = Dense(NO_CLASSES, activation='softmax')(dp2)\n",
    "    \n",
    "    model = Model(input, out)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    early_stoppage = EarlyStopping(monitor=\"loss\",mode=\"auto\", patience = 5,  restore_best_weights=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    print(\"---------------fold {} -----------\".format(i))\n",
    "    \n",
    "    model.fit(x_train[train], y_train[train], epochs=epochs,  batch_size=batch_size, callbacks=[early_stoppage],validation_data=(x_train[test],y_train[test]))\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_val_pred = model.predict(x_train[test])\n",
    "    y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "    y_val_true_classes = np.argmax(y_train[test], axis=1)\n",
    "\n",
    "        # Calculate accuracy for the fold\n",
    "    test_accuracy = accuracy_score(y_val_true_classes, y_val_pred_classes)\n",
    "    # all_accuracies.append(test_accuracy)\n",
    "    all_accuracies.append(test_accuracy)\n",
    "    # average_accuracy = np.mean(all_accuracies)\n",
    "\n",
    "\n",
    "    ##avg_acc = all_accuracies/epochs\n",
    "\n",
    "    print(f'Validation Accuracy Fold {i}: {test_accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_val_pred = model.predict(x_val)\n",
    "    y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "    y_val_true_classes = np.argmax(y_val, axis=1)\n",
    "    test_accuracy = accuracy_score(y_val_true_classes, y_val_pred_classes)\n",
    "    if test_accuracy >bestTestAccuracy:\n",
    "        bestTestAccuracy=test_accuracy\n",
    "        bestTest=i\n",
    "    print(f'Test - Accuracy: {test_accuracy}')\n",
    "\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_val_true_classes,y_val_pred_classes)\n",
    "\n",
    "    conf_matrixes.append(conf_mat)\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "\n",
    "average_accuracy = np.mean(all_accuracies)\n",
    "print(\"Average  cross validation accuracy: {average_accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "average_accuracy = np.mean(test_accuracies)\n",
    "print(\"Average test accuracy: {average_accuracy}\")\n",
    "\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# y_val_pred = model.predict(x_val)\n",
    "# y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "# y_val_true_classes = np.argmax(y_val, axis=1)\n",
    "# test_accuracy = accuracy_score(y_val_true_classes, y_val_pred_classes)\n",
    "# print(f'Test - Accuracy: {test_accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accuracies)\n",
    "print(np.average(test_accuracies))\n",
    "print(all_accuracies)\n",
    "print(np.average(all_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(conf_matrixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_conf_mat = np.sum(conf_matrixes, axis=0)\n",
    "# average_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat= conf_matrixes[bestTest-1]\n",
    "print('test accuracies: ',test_accuracies, bestTest-1)\n",
    "print ('validation accuracies: ',all_accuracies)\n",
    "# average_conf_mat = np.mean(conf_matrixes, axis=0)\n",
    "# for conf_matrix in conf_matrixes:\n",
    "#     average_conf_mat +=  conf_matrix\n",
    "# y_val_true_classes,y_val_pred_classes\n",
    "ylabel =['W1','W2','W3','W4','W5','W6','W7','W8','W9','W10','W11','W12','W19','W20','W37','W38','W39','W40','W41','W42','W43','W44','W45','W46','W47','W48','W49','W50','W91','W92','W93','W94','W95','W96','W97','W98','W99','W100','W111','W112','W211','W212','W213','W214','W215','W216','W217','W218','W219','W220','W351','W352','W353','W354','W355','W356','W357','W358','W359','W360']\n",
    "print(len(ylabel))\n",
    "# ticklabels=np.unique(y_val_true_classes)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',cmap='Blues',xticklabels=ylabel,yticklabels=ylabel)\n",
    "plt.title(f'confusion matrix for Test data in fold #{bestTest} Accuracy: {test_accuracies[bestTest-1]}')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'HandPose_RightHand_LeftHandFlipped_acc_{bestTest-1}_{test_accuracies[bestTest-1]}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_val_pred.shape)\n",
    "# print(y_val_pred)\n",
    "# print(y_val_pred[0][10])\n",
    "\n",
    "# bestTest-1\n",
    "\n",
    "plt.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_pred = model.predict(x_val)\n",
    "# print(y_val_pred[0])\n",
    "# y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "# y_val_true_classes = np.argmax(y_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
